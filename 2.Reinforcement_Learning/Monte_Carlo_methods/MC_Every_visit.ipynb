{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9fd793",
   "metadata": {},
   "source": [
    "# Apply Monte Carlo(MC) method to environment\n",
    "- 발표자 : 최찬혁\n",
    "\n",
    "This code is a code that applies Monte Carlo(MC) method to environment \"FrozenLake-v1\" from OpenAI gym.\n",
    "\n",
    "In this code, we use **every-visit** MC method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4685921",
   "metadata": {},
   "source": [
    "## Monte Carlo(MC) method\n",
    "MC method is tabular updating and model-free\n",
    "\n",
    "\n",
    "- Goal : learn $q_{\\pi}$ from entire episodes of real experience under policy $\\pi$\n",
    "    - entire trajectory of an episode : $S_0 , A_0 , R_1, \\cdots, S_{T-1}, A_{T-1}, R_T$\n",
    "    \n",
    "    - return : $G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma ^{T-t-1}R_T$ \n",
    "    \n",
    "    - action-value function : $q_{\\pi} \\left( s, a\\right) = \\mathbb{E}_{\\pi} \\left[ G_t | S_t =s, A_t = a\\right]$\n",
    "\n",
    " \n",
    "- MC policy Evaluation uses **empirical mean** return instead of expected return\n",
    "\n",
    "\n",
    "- $Q\\left( s, a \\right) \\rightarrow q_{\\pi} \\left( s, a \\right)$ as $n\\left( s, a \\right) \\rightarrow \\infty$ by the law of large numbers with assumption of i.i.d returns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecd96a1a",
   "metadata": {},
   "source": [
    "There are 2 ways to compute empirical mean.\n",
    "\n",
    "For $x_1, x_2, \\cdots, x_n$, the empirical mean is $\\mu _{n} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}$.\n",
    "\n",
    "Since we have \n",
    "- $x_1 + x_2 + \\cdots + x_{n-1} = \\left( n-1 \\right) \\mu _{n-1}$ and\n",
    "- $x_1 + x_2 + \\cdots + x_{n} = n \\mu _{n}$,\n",
    "\n",
    "we obtain that $x_n = n\\mu_{n} - \\left( n-1 \\right) \\mu _{n-1}$, that is, $\\mu_{n} = \\mu_{n-1} + \\frac{1}{n}\\left( x_n - \\mu_{n}\\right)$  (incremental updates).\n",
    "\n",
    "Instead of using incremental updates, we can use the following update.\n",
    "- $\\mu_{n} = \\mu_{n-1} + \\alpha \\left( x_n - \\mu_{n}\\right)$ (constant-$\\alpha$ updates)\n",
    "\n",
    "The constant-$\\alpha$ update prioritizes more recent samples. \n",
    "It is preferable to use this method since recent samples are more valuable due to udpated policy.\n",
    "\n",
    "Likewise, we can update the Q-table by the following update equations:\n",
    "- $n(S_t, A_t) \\leftarrow n(S_t,A_t) + 1$ \n",
    "    - $Q(S_t, A_t)\\leftarrow Q(S_t,A_t) + \\frac{1}{n(S_t,A_t)}(G_t - Q(S_t, A_t))$\n",
    "- $Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha(G_t - Q(S_t, A_t))$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b150c3a3",
   "metadata": {},
   "source": [
    "A pseudo code of MC method is the following.\n",
    "\n",
    "![pseudo code](./MC_Algorithm.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030b618",
   "metadata": {},
   "source": [
    "## Environment(FrozenLake-v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817717c",
   "metadata": {},
   "source": [
    "This game involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H) by walking over the Frozen(F) lake. The agent may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
    "\n",
    "The game will be terminated if you reach **Goal(G) or Holes(H)**.\n",
    "\n",
    "Reward will be awarded $1$ if you reach **Goal(G)** and $0$ for otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b717bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f995d",
   "metadata": {},
   "source": [
    "During loading the environment, we can give various options.\n",
    "\n",
    "- desc : Used to specify custom map for frozen lake. (It means that we can decide the positions of Start, Hole, Frozen, and Goal.\n",
    "\n",
    "- map_name : ID to use any of the preloaded maps. (4 * 4 or 8 * 8)\n",
    "\n",
    "- is_slippery(boolean) :  If True will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions. (For example, if action is left and is_slippery is True, then move left(or up/down) with probability 1/3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad00b2",
   "metadata": {},
   "source": [
    "![FrozenLake-env-4X4](./Frozen_Lake_v1_4X4.JPG)\n",
    "![FrozenLake-env-8X8](./Frozen_Lake_v1_8X8.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe76b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84777b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da37854",
   "metadata": {},
   "source": [
    "Observation will be an integer in $\\left\\{ 0, 1, \\cdots , 15\\right\\}$.\n",
    "\n",
    "This number means the location of character.\n",
    "\n",
    "![Location](./observation_space.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeba9bc",
   "metadata": {},
   "source": [
    "For example, initial observation must be $0$ and the env will be terminated if observation is $5,7,11,12$ and $15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f66bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd5a7a",
   "metadata": {},
   "source": [
    "Action will be an integer $0, 1, 2$ and $3$.\n",
    "\n",
    "$0$ : LEFT\n",
    "\n",
    "$1$ : DOWN\n",
    "\n",
    "$2$ : RIGHT\n",
    "\n",
    "$3$ : UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220d7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ac17689",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7111d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_initial = 1.0\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.05\n",
    "MAX_EPISODE = 20000\n",
    "GAMMA = 0.95\n",
    "\n",
    "# alpha\n",
    "step_size = 0.02 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9372a8dd",
   "metadata": {},
   "source": [
    "## Incremental Monte Carlo updates\n",
    "\n",
    "- $n\\left( S, A\\right) \\leftarrow n\\left( S, A\\right) + 1$\n",
    "- $Q\\left( S, A\\right) \\leftarrow Q\\left( S, A\\right) + \\frac{1}{n\\left( S, A\\right)} \\left[ G - Q\\left( S, A\\right)\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b27bcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1 = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "env1.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5511221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table1 = np.zeros((env1.observation_space.n, env1.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46983b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcc3c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_table = np.zeros((env1.observation_space.n, env1.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c7ce2",
   "metadata": {},
   "source": [
    "### $\\epsilon$-greedy\n",
    "Choose the greedy action with probability $1-\\epsilon$ and a random action with probability $\\epsilon$. (Same probability for each actions)\n",
    "\n",
    "(c) part of the algorithm figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98d9101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action1(Q_table, state, epsilon):\n",
    "    tmp = random.random()\n",
    "    if tmp < epsilon: # random action with probability epsilon\n",
    "        return np.random.randint(env1.action_space.n)\n",
    "    # greedy action with probability 1 - epsilon\n",
    "    return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d089c268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num:0\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "episode_num:500\n",
      "[[1 2 1 2]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:1000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:1500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:2000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:2500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:3000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:3500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:4000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:4500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:5000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:5500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:6000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:6500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:7000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:7500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:8000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:8500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:9000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:9500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:10000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:10500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:11000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:11500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:12000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:12500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:13000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:13500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:14000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:14500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:15000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:15500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:16000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:16500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:17000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:17500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:18000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:18500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:19000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:19500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(MAX_EPISODE):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    terminated = False # To start an episode\n",
    "    epsilon = np.max([epsilon_decay ** episode, epsilon_min])\n",
    "    \n",
    "    state,_ = env1.reset()\n",
    "    \n",
    "    # Part (a) of the pseudo-code above\n",
    "    while not terminated: \n",
    "        states.append(state)\n",
    "        action = get_action1(Q_table1, state, epsilon)\n",
    "        actions.append(action)\n",
    "        next_state, reward, terminated,_,_ = env1.step(action)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "    \n",
    "    G = 0\n",
    "    T = len(states)\n",
    "    \n",
    "    # Part (b) of the pseudo-code above\n",
    "    for t in reversed(range(T)): \n",
    "        G = (GAMMA * G) + rewards[t]\n",
    "        n_table[states[t], actions[t]] = n_table[states[t], actions[t]] + 1\n",
    "        Q_table1[states[t], actions[t]] = Q_table1[states[t], actions[t]] + ((G - Q_table1[states[t], actions[t]])/n_table[states[t], actions[t]])\n",
    "        \n",
    "    if episode % 500 == 0: # print log\n",
    "        print(\"episode_num:\" + str(episode))\n",
    "        Q_table_transform1 = np.argmax(Q_table1, axis=1)\n",
    "        Q_table_transform1 = np.reshape(Q_table_transform1, (4,4))\n",
    "        print(Q_table_transform1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac8ab2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, 0],\n",
       "       [1, 0, 1, 0],\n",
       "       [2, 1, 1, 0],\n",
       "       [0, 2, 2, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(np.argmax(Q_table1, axis=1), (4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02401b88",
   "metadata": {},
   "source": [
    "![Result_arrow_form](./Result_1.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "264a05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11f0a4",
   "metadata": {},
   "source": [
    "## Constant-$\\alpha$ Monte Carlo updates\n",
    "$Q\\left( S, A\\right) \\leftarrow Q\\left( S, A\\right) + \\alpha \\left[ G - Q\\left( S, A\\right)\\right] = \\alpha G + \\left( 1 - \\alpha \\right)Q\\left( S, A\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "561c0c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2 = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "env2.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a46da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table2 = np.zeros((env2.observation_space.n, env2.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d46c06bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action2(Q_table, state, epsilon):\n",
    "    tmp = random.random()\n",
    "    if tmp < epsilon: # random action with probability epsilon\n",
    "        return np.random.randint(env2.action_space.n)\n",
    "    # greedy action with probability 1 - epsilon    \n",
    "    return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "741e0928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_num:0\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "episode_num:500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:1000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:1500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:2000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:2500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:3000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:3500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:4000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:4500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:5000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:5500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:6000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:6500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:7000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:7500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:8000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:8500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 2 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:9000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:9500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 2 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:10000\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 2 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:10500\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:11000\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 2 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:11500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 2 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:12000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 2 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:12500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:13000\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:13500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:14000\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 2 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:14500\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 2 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:15000\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 2 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:15500\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:16000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:16500\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:17000\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:17500\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:18000\n",
      "[[2 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:18500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:19000\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "episode_num:19500\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(MAX_EPISODE):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    terminated = False # To start an episode\n",
    "    epsilon = np.max([epsilon_decay ** episode, epsilon_min])\n",
    "    \n",
    "    state,_ = env2.reset()\n",
    "\n",
    "    # Part (a) of the Pseudo-code\n",
    "    while not terminated: \n",
    "        states.append(state)\n",
    "        action = get_action2(Q_table2, state, epsilon)\n",
    "        actions.append(action)\n",
    "        next_state, reward, terminated, _, _ = env2.step(action)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "    G = 0\n",
    "    T = len(states)\n",
    "    \n",
    "    # Part (b) of the Pseudo-code\n",
    "    for t in reversed(range(T)): \n",
    "        G = (GAMMA * G) + rewards[t]\n",
    "        Q_table2[states[t], actions[t]] = (step_size * G) + ((1 - step_size) * Q_table2[states[t], actions[t]])\n",
    "        \n",
    "    if episode % 500 == 0:\n",
    "        print(\"episode_num:\" + str(episode))\n",
    "        Q_table_transform2 = np.argmax(Q_table2, axis=1)\n",
    "        Q_table_transform2 = np.reshape(Q_table_transform2, (4,4))\n",
    "        print(Q_table_transform2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e361149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 1, 0],\n",
       "       [1, 0, 1, 0],\n",
       "       [2, 1, 1, 0],\n",
       "       [0, 2, 2, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(np.argmax(Q_table2, axis=1), (4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ba3fd",
   "metadata": {},
   "source": [
    "![Result_arrow_form](./Result_2.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "563238f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ad989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathdrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
