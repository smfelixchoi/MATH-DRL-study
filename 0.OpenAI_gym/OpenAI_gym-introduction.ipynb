{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "befc2713",
   "metadata": {},
   "source": [
    "# OpenAI Gym package\n",
    "\n",
    "[Gym Documentation](https://www.gymlibrary.dev/)\n",
    "- 발표자 : 통합과정 최찬혁\n",
    "\n",
    "- OpenAI gym environment has changed into `gymnasium` from version 0.26.0.\n",
    "- In this page, the codes are based on the versino `gymnasium==0.28.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005b5d1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "**Reinforcement learning (RL)** is an area of machine learning concerned with how intelligent agents ought to take actions in an **environment** in order to maximize the notion of cumulative reward.\n",
    "\n",
    "In particular, the code that represents the **environment** is complex.\n",
    "\n",
    "![Breakout](./breakout.gif)\n",
    "\n",
    "OpenAI Gym pakage helps you configure your environment or bring up and visualize an environment that already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d4b86",
   "metadata": {},
   "source": [
    "## Core\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d7c1f75",
   "metadata": {},
   "source": [
    "### Spaces\n",
    "Spaces are usually used to specify the format of valid `actions` and `observations`. There are lots of space types available in OpenAI Gym.\n",
    "\n",
    "- **Box** : describes an n-dimensional **continuous** space. (For example, $\\left[ 0, 1 \\right] ^{3}$ )\n",
    "\n",
    "- **Discrete** : describes a **discrete** space. (For example, $\\left\\{ 0, 1, \\cdots , n-1 \\right\\} $, or $\\left\\{ a, a+1, \\cdots , a+n-1 \\right\\} $)\n",
    "\n",
    "- MultiBinary : creates a n-shape binary space.\n",
    "\n",
    "- MultiDiscrete : consists of a series of Discrete action spaces with a different number of actions in each element.\n",
    "\n",
    "- Dict : represents a dictionary of simple spaces.\n",
    "\n",
    "- Tuple : represents a tuple of simple spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e730499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete, Dict, Tuple, MultiBinary, MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8a8bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7875223  0.9110147]\n",
      " [ 1.5207179  1.6957202]]\n"
     ]
    }
   ],
   "source": [
    "observation_space1 = Box(low=-1.0, high=2.0, shape=(2,2), dtype=np.float32) #[-1, 2]^(2*2)\n",
    "print(observation_space1.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930feb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "observation_space2 = Discrete(4, start=-2) # start will be 0 for default\n",
    "print(observation_space2.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51fb17a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1]\n",
      " [1 1 0 1]\n",
      " [0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "observation_space3 = MultiBinary([3,4])\n",
    "print(observation_space3.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0da93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "observation_space4 = MultiDiscrete([5, 3, 2]) #{0,1,2,3,4}*{0,1,2}*{0,1}\n",
    "print(observation_space4.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d06bec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('state', 0), ('velocity', array([24.13437], dtype=float32))])\n"
     ]
    }
   ],
   "source": [
    "observation_space5 = Dict({\"state\": Discrete(3), \"velocity\": Box(low=0.0, high=100.0, shape=(1,), dtype=np.float32)})\n",
    "print(observation_space5.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d402eac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([36.808765], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "observation_space6 = Tuple((Discrete(4), Box(low=0.0, high=100.0, shape=(1,), dtype=np.float32)))\n",
    "print(observation_space6.sample())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ef23d9a",
   "metadata": {},
   "source": [
    "## gym.Env\n",
    "We can make an environment by `gym.make`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e517450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gymnasium.wrappers.time_limit.TimeLimit'>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(type(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e107a7",
   "metadata": {},
   "source": [
    "We can find action_space/observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da610c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LEFT/RIGHT\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e903e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The whole Observation Space\n",
    "# 4-dimensional observation space (Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity)\n",
    "\n",
    "env.observation_space\n",
    "\n",
    "# prints out [min values], [max values], dimension, dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf32310",
   "metadata": {},
   "source": [
    "### gym.Env.reset()\n",
    "Resets the environment to an initial state and returns the initial observation.\n",
    "\n",
    "Input : None\n",
    "\n",
    "Output : tuple consisting of (observation, info)\n",
    "\n",
    "- observation : initial observation\n",
    "\n",
    "- info : info contains auxiliary diagnostic information\n",
    "\n",
    "***Remark***. In the past version( < 0.26.0), it will return only observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc2d1b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.03620116,  0.01804511, -0.03028795, -0.03348435], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339834e",
   "metadata": {},
   "source": [
    "### gym.Env.step(action)\n",
    "Run one timestep of the environment’s dynamics.\n",
    "\n",
    "Input : (valid) action (it means that contained in the action space)\n",
    "\n",
    "Output : tuple consisting of (observation, reward, terminated, truncated, info)\n",
    "\n",
    "- observation : this will be an element of the observation_space.\n",
    "\n",
    "- reward : The amount of reward returned as a result of taking the action.\n",
    "\n",
    "- terminated : whether a terminal state (as defined under the MDP of the task) is reached. In this case further step() calls could return undefined results. (For example, gameover)\n",
    "\n",
    "- truncated : whether a truncation condition outside the scope of the MDP is satisfied. (For example, timelimit)\n",
    "\n",
    "- info : info contains auxiliary diagnostic information. (Useful for debugging)\n",
    "\n",
    "***Remark***. In the past version( < 0.26.0), it will return only 4 things(observation, reward, done, info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a7ef7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "3 (<class 'int'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Invalid action\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m a \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(\u001b[39m3\u001b[39;49m) \n",
      "File \u001b[0;32m/opt/anaconda3/envs/mathdrl/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mathdrl/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mathdrl/lib/python3.8/site-packages/gymnasium/wrappers/env_checker.py:47\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mathdrl/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:237\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    238\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    239\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[1;32m    240\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects step result to be a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mathdrl/lib/python3.8/site-packages/gymnasium/envs/classic_control/cartpole.py:133\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 133\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(\n\u001b[1;32m    134\u001b[0m         action\n\u001b[1;32m    135\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(action)\u001b[39m}\u001b[39;00m\u001b[39m) invalid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCall reset before using step method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m     x, x_dot, theta, theta_dot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "\u001b[0;31mAssertionError\u001b[0m: 3 (<class 'int'>) invalid"
     ]
    }
   ],
   "source": [
    "# Invalid action\n",
    "a = env.step(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f0e5045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03584026,  0.21358801, -0.03095764, -0.33556747], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valid action\n",
    "a = env.step(1) \n",
    "\n",
    "a\n",
    "# print (next state, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349b3679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:  [-0.03584026  0.21358801 -0.03095764 -0.33556747]\n",
      "reward:  1.0\n",
      "terminated:  False\n",
      "truncated:  False\n",
      "info:  {}\n"
     ]
    }
   ],
   "source": [
    "observation, reward, terminated, truncated, info = a\n",
    "print(\"observation: \", observation)\n",
    "print(\"reward: \", reward)\n",
    "print(\"terminated: \", terminated)\n",
    "print(\"truncated: \", truncated)\n",
    "print(\"info: \", info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dea2e4c4",
   "metadata": {},
   "source": [
    "## Load an environment that already exists (Atari;Breakout-v5)\n",
    "OpenAI gym package can load an environment that already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f6dd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, you must install \"pip install gym[atari]\", \"pip install gym[accept-rom-license]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc36f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env2 = gym.make(\"ALE/Breakout-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fca2504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0958d543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7944e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation2, info2 = env2.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1da5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation\n",
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "(210, 160, 3)\n",
      "info\n",
      "{'lives': 5, 'episode_frame_number': 0, 'frame_number': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"observation\")\n",
    "print(observation2)\n",
    "print(observation2.shape)\n",
    "print(\"info\")\n",
    "print(info2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc7eda78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACu0lEQVR4nO3dsW0TYRiA4QS5RkxARcEIEQNYLpjGEzCBx0AMQGGloEQZBlEgRJEiygL+IZZ9d/bL85Sn090vvfl8v+SzcnMDAMDZ3c55s91u989zttvtbOcfa+rrj+51yjVfnWMxXK7VUjeec1Jfcv6xzjWpUzPBcYtN8LUbfSpc2mSb4DgTfITRdE7xjD8XExy32AQf+1c/9flLXXNqJhjgYt1e43OFl/MMjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4bvjKzqW9/snfjb72NcFxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdw3PDF94fNZs51cKLvg+MmOE7gOIHjBI4TOG64i35692vOdTARExwncJzAcQLHCRw33EX/fP1nznUwERMcJ3CcwHECxwkcN95Fv3+ccx2c6sfhwyY4TuA4geMEjhM4briL/vz0ds51cKL14LgJjhM4TuA4geMEjhvuoh+/fJpxGZxsffj3hSY4TuA4geMEjhM4briL/ra/m3MdnOjj2j+n/C8JHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdw3Orrm99Lr+E6PGw2R51/t99PtJKDPtzfHzxuguMEjhM4brX0Aq7GzM/UczHBcSY44ko/YAAAAAAAAFjUMyEkRxlHStXAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img = Image.fromarray(observation2)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6520aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation, reward, terminated, truncated, info = env2.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "166c3bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACyklEQVR4nO3dsW0TYRiA4QSlRkxARcEIEQNYLpjGEzCBx0AMQGGloEQeBlEgRJEiYgH/EMu+O/vN85Sn090vvfl8vxQnd3MDAMDZ3c55s+12+99zNpvNbOcfa+rrj+51yjVfnWMxXK67pW4856Q+5/xjnWtSp2aC4xab4Gs3+lS4tMk2wXEm+Aij6ZziGX8uJjhusQk+9qd+6vOXuubUTDDAxbq9xucKz+cZHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdw3PArO5f29U/+bfRrXxMcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxw2/+L5fr+dcByf6PjhuguMEjhM4TuA4geOGu+ind7/mXAcTMcFxAscJHCdwnMBxw130z9d/5lwHEzHBcQLHCRwncJzAceNd9PvHOdfBqX4cPmyC4wSOEzhO4DiB44a76M9Pb+dcBydaDY6b4DiB4wSOEzhO4LjhLvrxy6cZl8HJVof/vtAExwkcJ3CcwHECxw130d9293OugxN9XHk55YskcJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcf3A+/V6v14vvYrF9AO/cALHCRw3/B8dGfe73dJLWJIJjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuC4u69vfi+9hutw7EuIZ35n4oeHh4PHTXCcwHECx/XfH3wuV/oeYhMcZ4IjrvQDBgAAAAAAgEX9BeNIS3MxDpCPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img2 = Image.fromarray(new_observation)\n",
    "display(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b7a81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation2, reward2, terminated2, truncated2, info2 = env2.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "718d0334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACy0lEQVR4nO3dvW0UURhAURs5RlRAREAJFgVYG1CNK6ACl4EogGDlgBC5GESAEIEDi4xoH3g1Oz97OSccjWaedP3tPMlrz8UFAAAnd7nkze7u7v55zu3t7WLnH2vu64/uNeWaL06xGLbraq0bLzmpzzn/WKea1LmZ4LjVJvjcjT4VtjbZJjjOBB9hNJ1zPONPxQTHrTbBx/7Uz33+WtecmwkG2KzLc3yu8HyewXECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxw2/srO1r3/yd6Nf+5rgOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOG74xfeH3W7JdTDR18FxExwncJzAcQLHCRw33EU/vfmx5DqYiQmOEzhO4DiB4wSOG+6iv7/8teQ6mIkJjhM4TuA4geMEjhvvot8+LrkOpvp2+LAJjhM4TuA4geMEjhvuoj8+vV5yHUx0MzhuguMEjhM4TuA4geOGu+jHTx8WXAaT3Rz++0ITHCdwnMBxAscJHDfcRX/ZXy+5DiZ6f+PllP8lgeMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjmsGftjtHna7tVexCc3A/CFwnMBxw/+Tddau9/u1l7AVJjhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB464+v/q59ho259j3Hm7hNU3v7u8PHjfBcQLHCRzXfHfhRFt4pp6KCY4zwRGlTx0AAAAAAACW8hvC6EtzS2QZPwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img3 = Image.fromarray(new_observation2)\n",
    "display(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9183b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation3, reward3, terminated3, truncated3, info3 = env2.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f46ed1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACy0lEQVR4nO3dMW7TYBiA4RZ1RpyAiYEjVBwgysBpegJOkGMgDsAQdWBEOQxiQIihQ9WVIT80Smwnb59ntCz7l95+8S81ra+uAAA4ues5b7bZbP57zt3d3WznH2rq64/udcw1X51iMZyvm6VuPOekPuf8Q51qUqdmguMWm+BLN/pUOLfJNsFxJvgAo+mc4hl/KiY4brEJPvSnfurzl7rm1EwwwNm6vsTnCs/nGRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncNzwKzvn9vVP/m30a18THCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscNv/i+W6/nXAdH+j44boLjBI4TOE7gOIHjhrvox3e/5lwHEzHBcQLHCRwncJzAccNd9M/Xf+ZcBxMxwXECxwkcJ3CcwHHjXfT7hznXwbF+7D9sguMEjhM4TuA4geOGu+jPj2/nXAdHWg2Om+A4geMEjhM4TuC44S764cunGZfB0Vb7/77QBMcJHCdwnMBxAscNd9HftrdzroMjfVx5OeWLJHCcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHGdwLv1erdeL72Ks9MJzF4CxwkcN/xflRfndrtdegnnyATHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncNzN1ze/l17D6R36AqXA+x4+3N/vPW6C4wSOEziu896kvwWeqadiguOaE/wC+dACAAAAAADgcE9mJ0tzOA336wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img3 = Image.fromarray(new_observation3)\n",
    "display(img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d48160",
   "metadata": {},
   "source": [
    "## Make a custom environment\n",
    "We can make a custom environment. It must contain \"step\", \"reset\" methods and observation_space, action_space\n",
    "\n",
    "1. \"step\" method\n",
    "\n",
    "    - Input : (valid) action\n",
    "    \n",
    "    - Output : ((valid) observation, reward, terminated, truncated, info)\n",
    "    \n",
    "2. \"reset\" method\n",
    "\n",
    "    - Input : seed, options\n",
    "    \n",
    "    - Output : ((valid) observation, info)\n",
    "    \n",
    "3. observation_space, action_space\n",
    "\n",
    "    - It must be gym.spaces type (For example, Discrete(4) or Box etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fae6d",
   "metadata": {},
   "source": [
    "Following code makes define simple GridWorld environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28c85f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to \n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]), # left\n",
    "            3: np.array([0, -1]), # down\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "        direction = self._action_to_direction[action]\n",
    "        # We use `np.clip` to make sure we don't leave the grid\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "        # An episode is done iff the agent has reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        reward = 1 if terminated else 0  # Binary sparse rewards\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info # truncated is False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d388abe",
   "metadata": {},
   "source": [
    "### Confirm your custom environment is well-defined\n",
    "We need to check whether custom environment is well-defined. We can check it by \"gym.utils.env_checker.check_env\".\n",
    "\n",
    "If a custom environment is well-defined, it won't print anything. However, it will print an error message if a custom environment isn't well-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5429ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.utils.env_checker import check_env\n",
    "custom_env = GridWorldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2ed9df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mathdrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "check_env(custom_env) # well-maed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68bf6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class has no \"step\" method so that this class isn't well-defined\n",
    "class GridWorldEnv2(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size  # The size of the square grid\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to \n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5405956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env2 = GridWorldEnv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5366bc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m check_env(custom_env2) \u001b[39m# Not well-made\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mathdrl/lib/python3.8/site-packages/gym/utils/env_checker.py:303\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39m# ============ Check the returned values ===============\u001b[39;00m\n\u001b[1;32m    302\u001b[0m env_reset_passive_checker(env)\n\u001b[0;32m--> 303\u001b[0m env_step_passive_checker(env, env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49msample())\n\u001b[1;32m    305\u001b[0m \u001b[39m# ==== Check the render method and the declared render modes ====\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_render_check:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mathdrl/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:214\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    215\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    216\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[1;32m    217\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects step result to be a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mathdrl/lib/python3.8/site-packages/gym/core.py:115\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[ObsType, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m     87\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run one timestep of the environment's dynamics.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[39m    When end of episode is reached, you are responsible for calling :meth:`reset` to reset this environment's state.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m            a certain timelimit was exceeded, or the physics simulation has entered an invalid state.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "check_env(custom_env2) # Not well-made"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d21a078",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c53d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathdrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
