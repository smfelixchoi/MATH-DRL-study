{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c334924d",
   "metadata": {},
   "source": [
    "# From Bellman Equation to Dynamic Programming\n",
    "\n",
    "- 발표자: 석사과정 최선묵"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77a830a2",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "- $\\mathcal{S}$: the state space\n",
    "- $\\mathcal{A}$: the action space\n",
    "- $P$ : the transition probability\n",
    "    - $P_{ss'}^a = P(S_{t+1}=s' | S_t =s, A_t = a)$\n",
    "    - Assume Stationary Markov Process, that is, $P_{ss'}^a$ does not depend on time step $t$.\n",
    "- $R$: the reward function\n",
    "- $\\gamma \\in [0,1]$: discount factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec194fc3",
   "metadata": {},
   "source": [
    "### The GOAL of Reinforcement Learning (Solving MDP)\n",
    "\n",
    "- Given an MDP, we aim to find an optimal `policy` $\\pi_\\ast \\colon \\mathcal{S} \\to \\mathcal{A}$.\n",
    "    - Policy decides which action the agent should choose for each state.\n",
    "    - Policy can be either deterministic or stochastic.\n",
    "        - Deterministic: $\\pi(s) = a \\in \\mathcal{A}$ for each $s \\in \\mathcal{S}$\n",
    "        - Stochastic: $\\pi(a|s)$ a distribution\n",
    "- `Optimal` in what sense?\n",
    "    - An optimal policy maximizes the expected total reward.\n",
    "- **Reward Hypothesis**\n",
    "    - All goals can be described by the maximization of the expected value of the cumulative sum of rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "223d3fc2",
   "metadata": {},
   "source": [
    "- Return $G_t$: the total discounted reward from time step $t$.\n",
    "    - $G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$, $\\gamma \\in [0,1]$\n",
    "- Why discount?\n",
    "    - mathematically convenient (converges if the reward function is bounded)\n",
    "    - uncertainty of the future\n",
    "    - immediate rewards may earn more interset than delayed rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88eef9d8",
   "metadata": {},
   "source": [
    "### Value functions\n",
    "\n",
    "- State-value function $v_\\pi(s)$ for policy $\\pi$\n",
    "$$ v_\\pi(s) = \\mathbb{E}_\\pi [G_t | S_t =s] $$\n",
    "\n",
    "- Action-value function $q_\\pi(s,a)$ for policy $\\pi$\n",
    "$$ q_\\pi(s,a) = \\mathbb{E}_\\pi [G_t | S_t=s, A_t =a]$$\n",
    "\n",
    "- By the definition, $v_\\pi(s) = \\sum_a \\pi(a|s) q_\\pi(s,a)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "553a3031",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation\n",
    "- a recursive equation decomposing value function into immediate reward and discounted successor value.\n",
    "\\begin{align*}\n",
    "v_\\pi(s) &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\\\\n",
    "q_\\pi(s,a) &= \\mathbb{E}_\\pi [R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t=a)]\n",
    "\\end{align*}\n",
    "\n",
    "### Optimal Value Functions and Policy\n",
    "- Optimal state-value function $v_\\ast(s) = \\max_\\pi v_\\pi(s)$\n",
    "- Optimal action-value function $q_\\ast(s,a) = \\max_\\pi q_\\pi(s,a)$\n",
    "- Note that $\\arg\\max_\\pi v_\\pi(s)$ and $\\arg\\max_\\pi v_\\pi(s')$ may not the same.\n",
    "\n",
    "#### Theorem\n",
    "- Define a partial ordering $\\pi' \\geq \\pi$ if $v_\\pi'(s) \\geq v_\\pi(s)$ for all $s$.\n",
    "    - There exists an optimal policy $\\pi_\\ast \\geq \\pi$ for all $\\pi$.\n",
    "    - All optimal policies achieve the optimal state-value function $v_{\\pi_\\ast}(s) = v_\\ast(s)$.\n",
    "    - All optimal policies achieve the optimal action-value function $q_{\\pi_\\ast}(s,a) = q_\\ast(s,a)$.\n",
    "- The theorem says that $$\\pi_\\ast = \\arg\\max_\\pi v_\\pi(s), \\quad \\forall s \\in \\mathcal{S}.$$\n",
    "\n",
    "#### Optimal Policy\n",
    "- An optimal policy can be found by maximizing over $q_\\ast(s,a)$.\n",
    "$$ \\pi_\\ast(a|s) = \\begin{cases} 1 & \\text{if} \\;\\; a = \\arg\\max_a q_\\ast(s,a) \\\\ 0 & \\text{otherwise} \\end{cases} \\quad \\text{or} \\quad \\pi_\\ast(s) = \\arg\\max_a q_\\ast(s,a)$$\n",
    "- $v_\\ast(s) = \\max_a q_\\ast(s,a)$\n",
    "- $v_\\ast(s)$ can be obtained directly from $q_\\ast(s,a)$\n",
    "- $q_\\ast(s,a)$ cannot be obtained directly from $v_\\ast(s)$.\n",
    "\n",
    "### Bellman Optimality Equation \n",
    "\n",
    "\\begin{align*}\n",
    "v_\\ast(s) &= \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma v_\\ast(s')] \\\\\n",
    "q_\\ast(s,a) &= \\sum_{s',r} p(s',r | s,a)[r+\\gamma \\max_{a'}q_\\ast(s',a')] \n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46ed9cd5",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "We want to find an `optimal value function` using iterative algorithm.\n",
    "- Bellman optimal equation\n",
    "    $$V^\\ast(s) = \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V^\\ast(s')]$$\n",
    "\n",
    "`Update Equation` : $V_{k+1}(s) \\leftarrow \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V_k(s')]$\n",
    "\n",
    "1. Initialize $V_0(s) = 0$ for all states $s$.\n",
    "2. Update $V_{k+1}(s)$ iteratively from all $V_k(s')$ (full backup) until convergence.\n",
    "    - synchronous backups: compute $V_{k+1}(s)$ for all $s$ and update simultaneously\n",
    "    - asynchronous backups: compute $V_{k+1}(s)$ for one $s$ and update it immediately\n",
    "3. Compute the optimal policy $\\pi_\\ast$\n",
    "$$ \\pi_\\ast(s) = \\arg\\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V^\\ast(s')]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dadba7b",
   "metadata": {},
   "source": [
    "### Example 1. Driving Car\n",
    "\n",
    "![image](./driving_car.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e776cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be5f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment \n",
    "# 3 states: cool(0), warm(1), overheated(2) \n",
    "# 2 actions: slow(0), fast(1)\n",
    "# Going faster gets double reward with one exception.\n",
    "\n",
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "P = {}\n",
    "P[0] = np.array([[[1,1], [0,1], [0,1]],\n",
    "                [[0.5,1], [0.5,1], [0,1]],\n",
    "                [[0,1], [0,1], [0,1]]])\n",
    "\n",
    "P[1] = np.array([[[0.5,2], [0.5,2], [0,2]],\n",
    "                [[0,2], [0,2], [1,-10]],\n",
    "                [[0,2], [0,2], [0,2]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8255282b",
   "metadata": {},
   "source": [
    "- `Update Equation` : $V_{k+1}(s) \\leftarrow \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V_k(s')]$\n",
    "- `Policy` : $ \\pi_\\ast(s) = \\arg\\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V^\\ast(s')]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37abc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value_ftn_vi(state, values, gamma):         # states: 0, 1, 2\n",
    "    temp_slow = 0\n",
    "    temp_fast = 0\n",
    "    for next_state in range(3):\n",
    "        temp_slow += P[0][state, next_state][0] * (P[0][state, next_state][1] + gamma * values[next_state])\n",
    "        temp_fast += P[1][state, next_state][0] * (P[1][state, next_state][1] + gamma * values[next_state])\n",
    "    \n",
    "    values = [temp_slow, temp_fast]\n",
    "    return max(values), np.argmax(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c7779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage: 0, V[0]= 0.592, V[1]= 0.404, V[2]=0.514, Policy: (1, 1, 1)\n",
      "stage: 1, V[0]= 2.448, V[1]= 1.448, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 2, V[0]= 3.753, V[1]= 2.753, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 3, V[0]= 4.928, V[1]= 3.928, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 4, V[0]= 5.985, V[1]= 4.985, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 5, V[0]= 6.937, V[1]= 5.937, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 6, V[0]= 7.793, V[1]= 6.793, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 7, V[0]= 8.564, V[1]= 7.564, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 8, V[0]= 9.257, V[1]= 8.257, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 9, V[0]= 9.881, V[1]= 8.881, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 10, V[0]= 10.443, V[1]= 9.443, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 11, V[0]= 10.949, V[1]= 9.949, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 12, V[0]= 11.404, V[1]= 10.404, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 13, V[0]= 11.814, V[1]= 10.814, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 14, V[0]= 12.182, V[1]= 11.182, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 15, V[0]= 12.514, V[1]= 11.514, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 16, V[0]= 12.813, V[1]= 11.813, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 17, V[0]= 13.081, V[1]= 12.081, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 18, V[0]= 13.323, V[1]= 12.323, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 19, V[0]= 13.541, V[1]= 12.541, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 20, V[0]= 13.737, V[1]= 12.737, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 21, V[0]= 13.913, V[1]= 12.913, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 22, V[0]= 14.072, V[1]= 13.072, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 23, V[0]= 14.215, V[1]= 13.215, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 24, V[0]= 14.343, V[1]= 13.343, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 25, V[0]= 14.459, V[1]= 13.459, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 26, V[0]= 14.563, V[1]= 13.563, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 27, V[0]= 14.657, V[1]= 13.657, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 28, V[0]= 14.741, V[1]= 13.741, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 29, V[0]= 14.817, V[1]= 13.817, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 30, V[0]= 14.885, V[1]= 13.885, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 31, V[0]= 14.947, V[1]= 13.947, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 32, V[0]= 15.002, V[1]= 14.002, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 33, V[0]= 15.052, V[1]= 14.052, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 34, V[0]= 15.097, V[1]= 14.097, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 35, V[0]= 15.137, V[1]= 14.137, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 36, V[0]= 15.173, V[1]= 14.173, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 37, V[0]= 15.206, V[1]= 14.206, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 38, V[0]= 15.235, V[1]= 14.235, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 39, V[0]= 15.262, V[1]= 14.262, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 40, V[0]= 15.286, V[1]= 14.286, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 41, V[0]= 15.307, V[1]= 14.307, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 42, V[0]= 15.326, V[1]= 14.326, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 43, V[0]= 15.344, V[1]= 14.344, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 44, V[0]= 15.359, V[1]= 14.359, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 45, V[0]= 15.373, V[1]= 14.373, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 46, V[0]= 15.386, V[1]= 14.386, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 47, V[0]= 15.397, V[1]= 14.397, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 48, V[0]= 15.408, V[1]= 14.408, V[2]=0.000, Policy: (1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "for i in range(3):\n",
    "    V[i] = np.random.random()\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.01\n",
    "gamma = 0.9\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(3):\n",
    "    Pi[i] = int(np.random.randint(low=0, high=2, size=1))\n",
    "\n",
    "stage = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    print('stage: %d, V[0]= %.3f, V[1]= %.3f, V[2]=%.3f, Policy: (%d, %d, %d)' %(stage, V[0], V[1], V[2], Pi[0], Pi[1], Pi[2]))\n",
    "    temp = []\n",
    "    for i in range(3):\n",
    "        temp.append(V[i])\n",
    "    for j in range(3):\n",
    "        v = V[j]\n",
    "        V[j], Pi[j] = next_value_ftn_vi(j, temp, gamma)\n",
    "        delta = max(delta, abs(v - V[j]))\n",
    "        \n",
    "    if delta < threshold:\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c1c4cd8",
   "metadata": {},
   "source": [
    "### Example 2. Grid World\n",
    "\n",
    "- States (Total 11 states)\n",
    "    - 0 : (1,1)\n",
    "    - 1 : (2,1)\n",
    "    - 2 : (3,1)\n",
    "    - 3 : (4,1)\n",
    "    - 4 : (1,2)\n",
    "    - 5 : (3,2)\n",
    "    - 6 : (4,2) -> **Terminal state**\n",
    "    - 7 : (1,3)\n",
    "    - 8 : (2,3)\n",
    "    - 9 : (3,3) \n",
    "    - 10 : (4,3) -> **Terminal state**\n",
    "- Actions (Total 4 actions)\n",
    "    - 0 : North\n",
    "    - 1 : South\n",
    "    - 2 : East\n",
    "    - 3 : West\n",
    "- Noisy environment, e.g., $P_{(3,1)(3,2)}^{North} = 0.8$, $P_{(3,1)(2,1)}^{North} = 0.1$, $P_{(3,1)(4,1)}^{North} = 0.1$\n",
    "- Reward\n",
    "    - When an agent reaches the state (4,2), then it gains $-1$ reward.\n",
    "    - When an agent reaches the state (4,3), then it gains $+1$ reward.\n",
    "    - An agent gains small negative reward $c$ for each step, except for the case when the agent reaches the terminal state.\n",
    "\n",
    "\n",
    "![image](./grid_world.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb788fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 4\n",
    "c=-0.4\n",
    "\n",
    "Action = {}\n",
    "Action[0] = 'North'\n",
    "Action[1] = 'South'\n",
    "Action[2] = 'East'\n",
    "Action[3] = 'West'\n",
    "\n",
    "P = {}\n",
    "for i in range(action_dim):\n",
    "    P[i] = np.zeros((state_dim,state_dim,2))\n",
    "\n",
    "# North\n",
    "P[0][0,0] = [0.1, c]\n",
    "P[0][0,1] = [0.1, c]\n",
    "P[0][0,4] = [0.8, c]\n",
    "P[0][1,0] = [0.1, c]\n",
    "P[0][1,1] = [0.8, c]\n",
    "P[0][1,2] = [0.1, c]\n",
    "P[0][2,1] = [0.1, c]\n",
    "P[0][2,5] = [0.8, c]\n",
    "P[0][2,3] = [0.1, c]\n",
    "P[0][3,2] = [0.1, c]\n",
    "P[0][3,3] = [0.1, c]\n",
    "P[0][3,6] = [0.8, -1]\n",
    "P[0][4,4] = [0.2, c]\n",
    "P[0][4,7] = [0.8, c]\n",
    "P[0][5,5] = [0.1, c]\n",
    "P[0][5,9] = [0.8, c]\n",
    "P[0][5,6] = [0.1, -1]\n",
    "P[0][7,7] = [0.9, c]\n",
    "P[0][7,8] = [0.1, c]\n",
    "P[0][8,7] = [0.1, c]\n",
    "P[0][8,8] = [0.8, c]\n",
    "P[0][8,9] = [0.1, c]\n",
    "P[0][9,8] = [0.1, c]\n",
    "P[0][9,9] = [0.8, c]\n",
    "P[0][9,10] = [0.1, 1]\n",
    "\n",
    "# South\n",
    "P[1][0,0] = [0.9, c]\n",
    "P[1][0,1] = [0.1, c]\n",
    "P[1][1,0] = [0.1, c]\n",
    "P[1][1,1] = [0.8, c]\n",
    "P[1][1,2] = [0.1, c]\n",
    "P[1][2,1] = [0.1, c]\n",
    "P[1][2,2] = [0.8, c]\n",
    "P[1][2,3] = [0.1, c]\n",
    "P[1][3,2] = [0.1, c]\n",
    "P[1][3,3] = [0.9, c]\n",
    "P[1][4,0] = [0.8, c]\n",
    "P[1][4,4] = [0.2, c]\n",
    "P[1][5,2] = [0.8, c]\n",
    "P[1][5,5] = [0.1, c]\n",
    "P[1][5,6] = [0.1, -1]\n",
    "P[1][7,4] = [0.8, c]\n",
    "P[1][7,7] = [0.1, c]\n",
    "P[1][7,8] = [0.1, c]\n",
    "P[1][8,7] = [0.1, c]\n",
    "P[1][8,8] = [0.8, c]\n",
    "P[1][8,9] = [0.1, c]\n",
    "P[1][9,8] = [0.1, c]\n",
    "P[1][9,9] = [0.8, c]\n",
    "P[1][9,10] = [0.1, 1]\n",
    "\n",
    "# East\n",
    "P[2][0,0] = [0.1, c]\n",
    "P[2][0,1] = [0.8, c]\n",
    "P[2][0,4] = [0.1, c]\n",
    "P[2][1,1] = [0.2, c]\n",
    "P[2][1,2] = [0.8, c]\n",
    "P[2][2,2] = [0.1, c]\n",
    "P[2][2,3] = [0.8, c]\n",
    "P[2][2,5] = [0.1, c]\n",
    "P[2][3,3] = [0.9, c]\n",
    "P[2][3,6] = [0.1, -1]\n",
    "P[2][4,0] = [0.1, c]\n",
    "P[2][4,4] = [0.8, c]\n",
    "P[2][4,7] = [0.1, c]\n",
    "P[2][5,2] = [0.1, c]\n",
    "P[2][5,6] = [0.8, -1]\n",
    "P[2][5,9] = [0.1, c]\n",
    "P[2][7,4] = [0.1, c]\n",
    "P[2][7,7] = [0.1, c]\n",
    "P[2][7,8] = [0.8, c]\n",
    "P[2][8,8] = [0.2, c]\n",
    "P[2][8,9] = [0.8, c]\n",
    "P[2][9,5] = [0.1, c]\n",
    "P[2][9,9] = [0.1, c]\n",
    "P[2][9,10] = [0.8, 1]\n",
    "\n",
    "# West\n",
    "P[3][0,0] = [0.9, c]\n",
    "P[3][0,4] = [0.1, c]\n",
    "P[3][1,0] = [0.8, c]\n",
    "P[3][1,1] = [0.2, c]\n",
    "P[3][2,1] = [0.8, c]\n",
    "P[3][2,2] = [0.1, c]\n",
    "P[3][2,5] = [0.1, c]\n",
    "P[3][3,2] = [0.8, c]\n",
    "P[3][3,3] = [0.1, c]\n",
    "P[3][3,6] = [0.1, -1]\n",
    "P[3][4,0] = [0.1, c]\n",
    "P[3][4,4] = [0.8, c]\n",
    "P[3][4,7] = [0.1, c]\n",
    "P[3][5,2] = [0.1, c]\n",
    "P[3][5,5] = [0.8, c]\n",
    "P[3][5,9] = [0.1, c]\n",
    "P[3][7,4] = [0.1, c]\n",
    "P[3][7,7] = [0.9, c]\n",
    "P[3][8,7] = [0.8, c]\n",
    "P[3][8,8] = [0.2, c]\n",
    "P[3][9,5] = [0.1, c]\n",
    "P[3][9,8] = [0.8, c]\n",
    "P[3][9,9] = [0.1, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1010f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value_ftn_vi_2(state, values, gamma, state_dim, action_dim):         # states: 0 ~ 10, actions: 0 ~ 3\n",
    "    temp = {}\n",
    "    for a in range(action_dim):\n",
    "        temp[a] = 0\n",
    "        for next_state in range(state_dim):\n",
    "            temp[a] += P[a][state, next_state][0] * (P[a][state, next_state][1] + gamma * values[next_state])\n",
    "            \n",
    "    # return maximum value and the corresponding action\n",
    "    return max(temp.values()), max(temp, key=temp.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b811b80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1\n",
      "Stage 2\n",
      "Stage 3\n",
      "Stage 4\n",
      "Stage 5\n",
      "Stage 6\n",
      "Stage 7\n",
      "Stage 8\n",
      "Stage 9\n",
      "Stage 10\n",
      "V[0] = -1.1503, best action: North\n",
      "V[1] = -0.8943, best action: East\n",
      "V[2] = -0.4644, best action: North\n",
      "V[3] = -0.8721, best action: West\n",
      "V[4] = -0.7894, best action: North\n",
      "V[5] = 0.1308, best action: North\n",
      "V[6] = 0.0000, best action: North\n",
      "V[7] = -0.3446, best action: East\n",
      "V[8] = 0.2183, best action: East\n",
      "V[9] = 0.8041, best action: East\n",
      "V[10] = 0.0000, best action: North\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "for i in range(state_dim):\n",
    "    V[i] = np.random.random()\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.01\n",
    "gamma = 0.9\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(state_dim):\n",
    "    Pi[i] = int(np.random.randint(low=0, high=action_dim, size=1))\n",
    "\n",
    "stage = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    temp = []\n",
    "    for i in range(state_dim):\n",
    "        temp.append(V[i])\n",
    "    for j in range(state_dim):\n",
    "        v = V[j]\n",
    "        V[j], Pi[j] = next_value_ftn_vi_2(j, temp, gamma, state_dim, action_dim)\n",
    "        delta = max(delta, abs(v - V[j]))\n",
    "        \n",
    "    if delta < threshold:\n",
    "        for i in range(state_dim):\n",
    "            print('V[%d] = %.4f, best action: %s' %(i, V[i], Action[Pi[i]]))\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        print('Stage %d'%stage)\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc65a228",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Policy Iteration repeats `policy evaluation` and `policy improvement` until convergence\n",
    "\n",
    "`Policy Evaluation`: computing $V^\\pi$ from the deterministic policy $\\pi$ using **Bellman Expectation Equation**\n",
    "- `Update Equation`: $V_{k+1}(s) \\leftarrow \\sum_{s',r} p(s',r |s,\\pi(s))[r+\\gamma V_k(s')]$\n",
    "1. Initialize $V_0(s)=0$ for all states $s$.\n",
    "2. Update every $V_{k+1}(s)$ from all $V_k(s')$ until convergence to $V^\\pi$.\n",
    "\n",
    "`Policy Improvement`: improving $\\pi$ to $\\pi'$ by greedy policy based on $V^\\pi$.\n",
    "- `Update Equation`: $\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r+\\gamma V^\\pi(s')] = \\arg\\max_a Q^\\pi(s,a)$\n",
    "\n",
    "Notice that $Q^\\pi(s,\\pi'(s)) \\geq V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee99240b",
   "metadata": {},
   "source": [
    "### Policy Improvement Theorem\n",
    "\n",
    "Let $\\pi$ and $\\pi'$ be two policies. If $Q^\\pi(s,\\pi'(s)) \\geq V^\\pi(s)$ for all $s \\in \\mathcal{S}$, then $V^{\\pi'}(s) \\geq V^\\pi(s)$ for all $s \\in \\mathcal{S}$. This implies that $\\pi'$ is a better policy than $\\pi$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbd8a839",
   "metadata": {},
   "source": [
    "### Example 1. Driving Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e1ed34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c417ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment \n",
    "# 3 states: cool(0), warm(1), overheated(2) \n",
    "# 2 actions: slow(0), fast(1)\n",
    "# Going faster gets double reward with one exception.\n",
    "\n",
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "P = {}\n",
    "P[0] = np.array([[[1,1], [0,1], [0,1]],\n",
    "                [[0.5,1], [0.5,1], [0,1]],\n",
    "                [[0,1], [0,1], [0,1]]])\n",
    "\n",
    "P[1] = np.array([[[0.5,2], [0.5,2], [0,2]],\n",
    "                [[0,2], [0,2], [1,-10]],\n",
    "                [[0,2], [0,2], [0,2]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e310a754",
   "metadata": {},
   "source": [
    "- `Update value`: $V_{k+1}(s) \\leftarrow \\sum_{s',r} p(s',r |s,\\pi(s))[r+\\gamma V_k(s')]$\n",
    "- `Update policy`: $\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r+\\gamma V^\\pi(s')] = \\arg\\max_a Q^\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f04f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value_ftn_pi(state, values, action, gamma):         # states: 0, 1, 2   actions: 0, 1\n",
    "    temp = 0\n",
    "    for i in range(3):\n",
    "        temp += P[action][state, i][0] * (P[action][state, i][1] + gamma * values[i])\n",
    "    return temp\n",
    "\n",
    "def find_action(state, values, gamma):         # states: 0, 1, 2\n",
    "    temp_slow = 0\n",
    "    temp_fast = 0\n",
    "    for next_state in range(3):\n",
    "        temp_slow += P[0][state, next_state][0] * (P[0][state, next_state][1] + gamma * values[next_state])\n",
    "        temp_fast += P[1][state, next_state][0] * (P[1][state, next_state][1] + gamma * values[next_state])\n",
    "    \n",
    "    values = [temp_slow, temp_fast]\n",
    "    return np.argmax(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7904989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage: 0, V[0]= 0.323, V[1]= 0.171, V[2]=0.221, Policy: (0, 0, 0)\n",
      "Evaluation Phase\n",
      "counter: 1, V[0]= 1.291, V[1]= 1.223, V[2]=0.000\n",
      "counter: 2, V[0]= 2.162, V[1]= 2.131, V[2]=0.000\n",
      "counter: 3, V[0]= 2.946, V[1]= 2.932, V[2]=0.000\n",
      "counter: 4, V[0]= 3.651, V[1]= 3.645, V[2]=0.000\n",
      "counter: 5, V[0]= 4.286, V[1]= 4.283, V[2]=0.000\n",
      "counter: 6, V[0]= 4.857, V[1]= 4.856, V[2]=0.000\n",
      "counter: 7, V[0]= 5.372, V[1]= 5.371, V[2]=0.000\n",
      "counter: 8, V[0]= 5.835, V[1]= 5.834, V[2]=0.000\n",
      "counter: 9, V[0]= 6.251, V[1]= 6.251, V[2]=0.000\n",
      "counter: 10, V[0]= 6.626, V[1]= 6.626, V[2]=0.000\n",
      "counter: 11, V[0]= 6.963, V[1]= 6.963, V[2]=0.000\n",
      "counter: 12, V[0]= 7.267, V[1]= 7.267, V[2]=0.000\n",
      "counter: 13, V[0]= 7.540, V[1]= 7.540, V[2]=0.000\n",
      "counter: 14, V[0]= 7.786, V[1]= 7.786, V[2]=0.000\n",
      "counter: 15, V[0]= 8.008, V[1]= 8.008, V[2]=0.000\n",
      "counter: 16, V[0]= 8.207, V[1]= 8.207, V[2]=0.000\n",
      "counter: 17, V[0]= 8.386, V[1]= 8.386, V[2]=0.000\n",
      "counter: 18, V[0]= 8.548, V[1]= 8.548, V[2]=0.000\n",
      "counter: 19, V[0]= 8.693, V[1]= 8.693, V[2]=0.000\n",
      "counter: 20, V[0]= 8.824, V[1]= 8.824, V[2]=0.000\n",
      "counter: 21, V[0]= 8.941, V[1]= 8.941, V[2]=0.000\n",
      "counter: 22, V[0]= 9.047, V[1]= 9.047, V[2]=0.000\n",
      "counter: 23, V[0]= 9.142, V[1]= 9.142, V[2]=0.000\n",
      "counter: 24, V[0]= 9.228, V[1]= 9.228, V[2]=0.000\n",
      "counter: 25, V[0]= 9.305, V[1]= 9.305, V[2]=0.000\n",
      "counter: 26, V[0]= 9.375, V[1]= 9.375, V[2]=0.000\n",
      "counter: 27, V[0]= 9.437, V[1]= 9.437, V[2]=0.000\n",
      "counter: 28, V[0]= 9.494, V[1]= 9.494, V[2]=0.000\n",
      "counter: 29, V[0]= 9.544, V[1]= 9.544, V[2]=0.000\n",
      "counter: 30, V[0]= 9.590, V[1]= 9.590, V[2]=0.000\n",
      "counter: 31, V[0]= 9.631, V[1]= 9.631, V[2]=0.000\n",
      "counter: 32, V[0]= 9.668, V[1]= 9.668, V[2]=0.000\n",
      "counter: 33, V[0]= 9.701, V[1]= 9.701, V[2]=0.000\n",
      "counter: 34, V[0]= 9.731, V[1]= 9.731, V[2]=0.000\n",
      "counter: 35, V[0]= 9.758, V[1]= 9.758, V[2]=0.000\n",
      "counter: 36, V[0]= 9.782, V[1]= 9.782, V[2]=0.000\n",
      "counter: 37, V[0]= 9.804, V[1]= 9.804, V[2]=0.000\n",
      "counter: 38, V[0]= 9.823, V[1]= 9.823, V[2]=0.000\n",
      "counter: 39, V[0]= 9.841, V[1]= 9.841, V[2]=0.000\n",
      "counter: 40, V[0]= 9.857, V[1]= 9.857, V[2]=0.000\n",
      "counter: 41, V[0]= 9.871, V[1]= 9.871, V[2]=0.000\n",
      "counter: 42, V[0]= 9.884, V[1]= 9.884, V[2]=0.000\n",
      "counter: 43, V[0]= 9.896, V[1]= 9.896, V[2]=0.000\n",
      "counter: 44, V[0]= 9.906, V[1]= 9.906, V[2]=0.000\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "stage: 1, V[0]= 9.916, V[1]= 9.916, V[2]=0.000, Policy: (1, 0, 0)\n",
      "Evaluation Phase\n",
      "counter: 1, V[0]= 10.924, V[1]= 9.924, V[2]=0.000\n",
      "counter: 2, V[0]= 11.382, V[1]= 10.382, V[2]=0.000\n",
      "counter: 3, V[0]= 11.793, V[1]= 10.793, V[2]=0.000\n",
      "counter: 4, V[0]= 12.164, V[1]= 11.164, V[2]=0.000\n",
      "counter: 5, V[0]= 12.498, V[1]= 11.498, V[2]=0.000\n",
      "counter: 6, V[0]= 12.798, V[1]= 11.798, V[2]=0.000\n",
      "counter: 7, V[0]= 13.068, V[1]= 12.068, V[2]=0.000\n",
      "counter: 8, V[0]= 13.311, V[1]= 12.311, V[2]=0.000\n",
      "counter: 9, V[0]= 13.530, V[1]= 12.530, V[2]=0.000\n",
      "counter: 10, V[0]= 13.727, V[1]= 12.727, V[2]=0.000\n",
      "counter: 11, V[0]= 13.904, V[1]= 12.904, V[2]=0.000\n",
      "counter: 12, V[0]= 14.064, V[1]= 13.064, V[2]=0.000\n",
      "counter: 13, V[0]= 14.208, V[1]= 13.208, V[2]=0.000\n",
      "counter: 14, V[0]= 14.337, V[1]= 13.337, V[2]=0.000\n",
      "counter: 15, V[0]= 14.453, V[1]= 13.453, V[2]=0.000\n",
      "counter: 16, V[0]= 14.558, V[1]= 13.558, V[2]=0.000\n",
      "counter: 17, V[0]= 14.652, V[1]= 13.652, V[2]=0.000\n",
      "counter: 18, V[0]= 14.737, V[1]= 13.737, V[2]=0.000\n",
      "counter: 19, V[0]= 14.813, V[1]= 13.813, V[2]=0.000\n",
      "counter: 20, V[0]= 14.882, V[1]= 13.882, V[2]=0.000\n",
      "counter: 21, V[0]= 14.944, V[1]= 13.944, V[2]=0.000\n",
      "counter: 22, V[0]= 14.999, V[1]= 13.999, V[2]=0.000\n",
      "counter: 23, V[0]= 15.049, V[1]= 14.049, V[2]=0.000\n",
      "counter: 24, V[0]= 15.094, V[1]= 14.094, V[2]=0.000\n",
      "counter: 25, V[0]= 15.135, V[1]= 14.135, V[2]=0.000\n",
      "counter: 26, V[0]= 15.171, V[1]= 14.171, V[2]=0.000\n",
      "counter: 27, V[0]= 15.204, V[1]= 14.204, V[2]=0.000\n",
      "counter: 28, V[0]= 15.234, V[1]= 14.234, V[2]=0.000\n",
      "counter: 29, V[0]= 15.261, V[1]= 14.261, V[2]=0.000\n",
      "counter: 30, V[0]= 15.284, V[1]= 14.284, V[2]=0.000\n",
      "counter: 31, V[0]= 15.306, V[1]= 14.306, V[2]=0.000\n",
      "counter: 32, V[0]= 15.325, V[1]= 14.325, V[2]=0.000\n",
      "counter: 33, V[0]= 15.343, V[1]= 14.343, V[2]=0.000\n",
      "counter: 34, V[0]= 15.359, V[1]= 14.359, V[2]=0.000\n",
      "counter: 35, V[0]= 15.373, V[1]= 14.373, V[2]=0.000\n",
      "counter: 36, V[0]= 15.385, V[1]= 14.385, V[2]=0.000\n",
      "counter: 37, V[0]= 15.397, V[1]= 14.397, V[2]=0.000\n",
      "counter: 38, V[0]= 15.407, V[1]= 14.407, V[2]=0.000\n",
      "Improvement Phase\n",
      "Final Values and an optimal policy. V[0]= 15.4165, V[1]=14.4165, V[2]=0.0000, optimal policy: (1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "for i in range(3):\n",
    "    V[i] = np.random.random()\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.01\n",
    "gamma = 0.9\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(3):\n",
    "    Pi[i] = int(np.random.randint(low=0, high=2, size=1))\n",
    "\n",
    "stage = 0\n",
    "while True:\n",
    "    print('stage: %d, V[0]= %.3f, V[1]= %.3f, V[2]=%.3f, Policy: (%d, %d, %d)' %(stage, V[0], V[1], V[2], Pi[0], Pi[1], Pi[2]))\n",
    "\n",
    "    ########## Policy Evaluation ############\n",
    "    print('Evaluation Phase')\n",
    "    counter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        temp = []\n",
    "        for i in range(3):\n",
    "            temp.append(V[i])\n",
    "        for j in range(3):\n",
    "            v = V[j]\n",
    "            V[j] = next_value_ftn_pi(j, temp, Pi[j], gamma)\n",
    "            delta = max(delta, abs(v - V[j]))\n",
    "            \n",
    "        if delta < threshold:\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            print('counter: %d, V[0]= %.3f, V[1]= %.3f, V[2]=%.3f' %(counter, V[0], V[1], V[2]))\n",
    "            continue\n",
    "    #########################################\n",
    "\n",
    "    ########## Policy Improvement ###########\n",
    "    print('Improvement Phase')\n",
    "    policy_stable = True\n",
    "    for i in range(3):\n",
    "        old_action = Pi[i]\n",
    "        Pi[i] = find_action(i, V, gamma)\n",
    "        if old_action != Pi[i]:\n",
    "            policy_stable = False\n",
    "    \n",
    "    if policy_stable:\n",
    "        print('Final Values and an optimal policy. V[0]= %.4f, V[1]=%.4f, V[2]=%.4f, optimal policy: (%d, %d, %d)' %(V[0], V[1], V[2], Pi[0], Pi[1], Pi[2]))\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        print('Go back to Evaluation Phase.')\n",
    "        print('#'*50)\n",
    "        continue\n",
    "    #######################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adedca07",
   "metadata": {},
   "source": [
    "### Example 2. Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "738b9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 4\n",
    "c=-0.4\n",
    "\n",
    "Action = {}\n",
    "Action[0] = 'North'\n",
    "Action[1] = 'South'\n",
    "Action[2] = 'East'\n",
    "Action[3] = 'West'\n",
    "\n",
    "P = {}\n",
    "for i in range(action_dim):\n",
    "    P[i] = np.zeros((state_dim,state_dim,2))\n",
    "\n",
    "# North\n",
    "P[0][0,0] = [0.1, c]\n",
    "P[0][0,1] = [0.1, c]\n",
    "P[0][0,4] = [0.8, c]\n",
    "P[0][1,0] = [0.1, c]\n",
    "P[0][1,1] = [0.8, c]\n",
    "P[0][1,2] = [0.1, c]\n",
    "P[0][2,1] = [0.1, c]\n",
    "P[0][2,5] = [0.8, c]\n",
    "P[0][2,3] = [0.1, c]\n",
    "P[0][3,2] = [0.1, c]\n",
    "P[0][3,3] = [0.1, c]\n",
    "P[0][3,6] = [0.8, -1]\n",
    "P[0][4,4] = [0.2, c]\n",
    "P[0][4,7] = [0.8, c]\n",
    "P[0][5,5] = [0.1, c]\n",
    "P[0][5,9] = [0.8, c]\n",
    "P[0][5,6] = [0.1, -1]\n",
    "P[0][7,7] = [0.9, c]\n",
    "P[0][7,8] = [0.1, c]\n",
    "P[0][8,7] = [0.1, c]\n",
    "P[0][8,8] = [0.8, c]\n",
    "P[0][8,9] = [0.1, c]\n",
    "P[0][9,8] = [0.1, c]\n",
    "P[0][9,9] = [0.8, c]\n",
    "P[0][9,10] = [0.1, 1]\n",
    "\n",
    "# South\n",
    "P[1][0,0] = [0.9, c]\n",
    "P[1][0,1] = [0.1, c]\n",
    "P[1][1,0] = [0.1, c]\n",
    "P[1][1,1] = [0.8, c]\n",
    "P[1][1,2] = [0.1, c]\n",
    "P[1][2,1] = [0.1, c]\n",
    "P[1][2,2] = [0.8, c]\n",
    "P[1][2,3] = [0.1, c]\n",
    "P[1][3,2] = [0.1, c]\n",
    "P[1][3,3] = [0.9, c]\n",
    "P[1][4,0] = [0.8, c]\n",
    "P[1][4,4] = [0.2, c]\n",
    "P[1][5,2] = [0.8, c]\n",
    "P[1][5,5] = [0.1, c]\n",
    "P[1][5,6] = [0.1, -1]\n",
    "P[1][7,4] = [0.8, c]\n",
    "P[1][7,7] = [0.1, c]\n",
    "P[1][7,8] = [0.1, c]\n",
    "P[1][8,7] = [0.1, c]\n",
    "P[1][8,8] = [0.8, c]\n",
    "P[1][8,9] = [0.1, c]\n",
    "P[1][9,8] = [0.1, c]\n",
    "P[1][9,9] = [0.8, c]\n",
    "P[1][9,10] = [0.1, 1]\n",
    "\n",
    "# East\n",
    "P[2][0,0] = [0.1, c]\n",
    "P[2][0,1] = [0.8, c]\n",
    "P[2][0,4] = [0.1, c]\n",
    "P[2][1,1] = [0.2, c]\n",
    "P[2][1,2] = [0.8, c]\n",
    "P[2][2,2] = [0.1, c]\n",
    "P[2][2,3] = [0.8, c]\n",
    "P[2][2,5] = [0.1, c]\n",
    "P[2][3,3] = [0.9, c]\n",
    "P[2][3,6] = [0.1, -1]\n",
    "P[2][4,0] = [0.1, c]\n",
    "P[2][4,4] = [0.8, c]\n",
    "P[2][4,7] = [0.1, c]\n",
    "P[2][5,2] = [0.1, c]\n",
    "P[2][5,6] = [0.8, -1]\n",
    "P[2][5,9] = [0.1, c]\n",
    "P[2][7,4] = [0.1, c]\n",
    "P[2][7,7] = [0.1, c]\n",
    "P[2][7,8] = [0.8, c]\n",
    "P[2][8,8] = [0.2, c]\n",
    "P[2][8,9] = [0.8, c]\n",
    "P[2][9,5] = [0.1, c]\n",
    "P[2][9,9] = [0.1, c]\n",
    "P[2][9,10] = [0.8, 1]\n",
    "\n",
    "# West\n",
    "P[3][0,0] = [0.9, c]\n",
    "P[3][0,4] = [0.1, c]\n",
    "P[3][1,0] = [0.8, c]\n",
    "P[3][1,1] = [0.2, c]\n",
    "P[3][2,1] = [0.8, c]\n",
    "P[3][2,2] = [0.1, c]\n",
    "P[3][2,5] = [0.1, c]\n",
    "P[3][3,2] = [0.8, c]\n",
    "P[3][3,3] = [0.1, c]\n",
    "P[3][3,6] = [0.1, -1]\n",
    "P[3][4,0] = [0.1, c]\n",
    "P[3][4,4] = [0.8, c]\n",
    "P[3][4,7] = [0.1, c]\n",
    "P[3][5,2] = [0.1, c]\n",
    "P[3][5,5] = [0.8, c]\n",
    "P[3][5,9] = [0.1, c]\n",
    "P[3][7,4] = [0.1, c]\n",
    "P[3][7,7] = [0.9, c]\n",
    "P[3][8,7] = [0.8, c]\n",
    "P[3][8,8] = [0.2, c]\n",
    "P[3][9,5] = [0.1, c]\n",
    "P[3][9,8] = [0.8, c]\n",
    "P[3][9,9] = [0.1, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bae70020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value_ftn_pi_2(state, values, action, gamma, state_dim):         # states: 0~10   actions: 0~3\n",
    "    temp = 0\n",
    "    for next_state in range(state_dim):\n",
    "        temp += P[action][state, next_state][0] * (P[action][state, next_state][1] + gamma * values[next_state])\n",
    "    return temp\n",
    "\n",
    "def find_action_2(state, values, gamma, state_dim, action_dim):         # states: 0~10\n",
    "    temp = {}\n",
    "    for a in range(action_dim):\n",
    "        temp[a] = 0\n",
    "        for next_state in range(state_dim):\n",
    "            temp[a] += P[a][state, next_state][0] * (P[a][state, next_state][1] + gamma * values[next_state])\n",
    "            \n",
    "    # return the action that maximizes the value\n",
    "    return max(temp, key=temp.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "434d97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Phase Stage 1\n",
      "Current policy: (2, 0, 0, 2, 3, 3, 3, 0, 1, 3, 2)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Evaluation counter 5\n",
      "Evaluation counter 6\n",
      "Evaluation counter 7\n",
      "Evaluation counter 8\n",
      "Evaluation counter 9\n",
      "Evaluation counter 10\n",
      "Evaluation counter 11\n",
      "Evaluation counter 12\n",
      "Evaluation counter 13\n",
      "Evaluation counter 14\n",
      "Evaluation counter 15\n",
      "Evaluation counter 16\n",
      "Evaluation counter 17\n",
      "Evaluation counter 18\n",
      "Evaluation counter 19\n",
      "Evaluation counter 20\n",
      "Evaluation counter 21\n",
      "Evaluation counter 22\n",
      "Evaluation counter 23\n",
      "Evaluation counter 24\n",
      "Evaluation counter 25\n",
      "Evaluation counter 26\n",
      "Evaluation counter 27\n",
      "Evaluation counter 28\n",
      "Evaluation counter 29\n",
      "Evaluation counter 30\n",
      "Evaluation counter 31\n",
      "Evaluation counter 32\n",
      "Evaluation counter 33\n",
      "Evaluation counter 34\n",
      "Evaluation counter 35\n",
      "Evaluation counter 36\n",
      "Evaluation counter 37\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 2\n",
      "Current policy: (2, 2, 2, 0, 1, 2, 0, 1, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Evaluation counter 5\n",
      "Evaluation counter 6\n",
      "Evaluation counter 7\n",
      "Evaluation counter 8\n",
      "Evaluation counter 9\n",
      "Evaluation counter 10\n",
      "Evaluation counter 11\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 3\n",
      "Current policy: (2, 2, 0, 0, 1, 0, 0, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Evaluation counter 5\n",
      "Evaluation counter 6\n",
      "Evaluation counter 7\n",
      "Evaluation counter 8\n",
      "Evaluation counter 9\n",
      "Evaluation counter 10\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 4\n",
      "Current policy: (2, 2, 0, 3, 0, 0, 0, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 5\n",
      "Current policy: (0, 2, 0, 3, 0, 0, 0, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Improvement Phase\n",
      "V[0] = -1.1566, best action: North\n",
      "V[1] = -0.8964, best action: East\n",
      "V[2] = -0.4652, best action: North\n",
      "V[3] = -0.8736, best action: West\n",
      "V[4] = -0.7917, best action: North\n",
      "V[5] = 0.1307, best action: North\n",
      "V[6] = 0.0000, best action: North\n",
      "V[7] = -0.3453, best action: East\n",
      "V[8] = 0.2183, best action: East\n",
      "V[9] = 0.8041, best action: East\n",
      "V[10] = 0.0000, best action: North\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "for i in range(state_dim):\n",
    "    V[i] = np.random.random()\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.01\n",
    "gamma = 0.9\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(state_dim):\n",
    "    Pi[i] = int(np.random.randint(low=0, high=action_dim, size=1))\n",
    "\n",
    "stage = 1\n",
    "while True:\n",
    "    ########## Policy Evaluation ############\n",
    "    print('Evaluation Phase Stage %d' %stage)\n",
    "    print('Current policy:', (Pi[0], Pi[1], Pi[2], Pi[3], Pi[4], Pi[5], Pi[6], Pi[7], Pi[8], Pi[9], Pi[10]))\n",
    "    counter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        temp = []\n",
    "        for i in range(state_dim):\n",
    "            temp.append(V[i])\n",
    "        for j in range(state_dim):\n",
    "            v = V[j]\n",
    "            V[j] = next_value_ftn_pi_2(j, temp, Pi[j], gamma, state_dim)\n",
    "            delta = max(delta, abs(v - V[j]))\n",
    "            \n",
    "        if delta < threshold:\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            print('Evaluation counter %d' %counter)\n",
    "            continue\n",
    "    #########################################\n",
    "\n",
    "    ########## Policy Improvement ###########\n",
    "    print('Improvement Phase')\n",
    "    policy_stable = True\n",
    "    for i in range(state_dim):\n",
    "        old_action = Pi[i]\n",
    "        Pi[i] = find_action_2(i, V, gamma, state_dim, action_dim)\n",
    "        if old_action != Pi[i]:\n",
    "            policy_stable = False\n",
    "    \n",
    "    if policy_stable:\n",
    "        for i in range(state_dim):\n",
    "            print('V[%d] = %.4f, best action: %s' %(i, V[i], Action[Pi[i]]))\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        print('Go back to Evaluation Phase.')\n",
    "        print('#'*50)\n",
    "        continue\n",
    "    #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd3d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MathDLstudy",
   "language": "python",
   "name": "mathdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
