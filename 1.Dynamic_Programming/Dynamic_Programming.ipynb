{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c334924d",
   "metadata": {},
   "source": [
    "# From Bellman Equation to Dynamic Programming\n",
    "\n",
    "- 발표자: 석사과정 최선묵"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77a830a2",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "- $\\mathcal{S}$: the state space\n",
    "- $\\mathcal{A}$: the action space\n",
    "- $P$ : the transition probability\n",
    "    - $P_{ss'}^a = P(S_{t+1}=s' | S_t =s, A_t = a)$\n",
    "    - Assume Stationary Markov Process, that is, $P_{ss'}^a$ does not depend on time step $t$.\n",
    "- $R$: the reward function\n",
    "- $\\gamma \\in [0,1]$: discount factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec194fc3",
   "metadata": {},
   "source": [
    "### The GOAL of Reinforcement Learning (Solving MDP)\n",
    "\n",
    "- Given an MDP, we aim to find an optimal `policy` $\\pi_\\ast \\colon \\mathcal{S} \\to \\mathcal{A}$.\n",
    "    - Policy decides which action the agent should choose for each state.\n",
    "    - Policy can be either deterministic or stochastic.\n",
    "        - Deterministic: $\\pi(s) = a \\in \\mathcal{A}$ for each $s \\in \\mathcal{S}$\n",
    "        - Stochastic: $\\pi(a|s)$ a distribution\n",
    "- `Optimal` in what sense?\n",
    "    - An optimal policy maximizes the expected total reward.\n",
    "- **Reward Hypothesis**\n",
    "    - All goals can be described by the maximization of the expected value of the cumulative sum of rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "223d3fc2",
   "metadata": {},
   "source": [
    "- Return $G_t$: the total discounted reward from time step $t$.\n",
    "    - $G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$, $\\gamma \\in [0,1]$\n",
    "- Why discount?\n",
    "    - mathematically convenient (converges if the reward function is bounded)\n",
    "    - uncertainty of the future\n",
    "    - immediate rewards may earn more interset than delayed rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88eef9d8",
   "metadata": {},
   "source": [
    "### Value functions\n",
    "\n",
    "- State-value function $v_\\pi(s)$ for policy $\\pi$\n",
    "$$ v_\\pi(s) = \\mathbb{E}_\\pi [G_t | S_t =s] $$\n",
    "\n",
    "- Action-value function $q_\\pi(s,a)$ for policy $\\pi$\n",
    "$$ q_\\pi(s,a) = \\mathbb{E}_\\pi [G_t | S_t=s, A_t =a]$$\n",
    "\n",
    "- By the definition, $v_\\pi(s) = \\sum_a \\pi(a|s) q_\\pi(s,a)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "553a3031",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation\n",
    "- a recursive equation decomposing value function into immediate reward and discounted successor value.\n",
    "\\begin{align*}\n",
    "v_\\pi(s) &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\\\\n",
    "q_\\pi(s,a) &= \\mathbb{E}_\\pi [R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t=a)]\n",
    "\\end{align*}\n",
    "\n",
    "### Optimal Value Functions and Policy\n",
    "- Optimal state-value function $v_\\ast(s) = \\max_\\pi v_\\pi(s)$\n",
    "- Optimal action-value function $q_\\ast(s,a) = \\max_\\pi q_\\pi(s,a)$\n",
    "- Note that $\\arg\\max_\\pi v_\\pi(s)$ and $\\arg\\max_\\pi v_\\pi(s')$ may not the same.\n",
    "\n",
    "#### Theorem\n",
    "- Define a partial ordering $\\pi' \\geq \\pi$ if $v_\\pi'(s) \\geq v_\\pi(s)$ for all $s$.\n",
    "    - There exists an optimal policy $\\pi_\\ast \\geq \\pi$ for all $\\pi$.\n",
    "    - All optimal policies achieve the optimal state-value function $v_{\\pi_\\ast}(s) = v_\\ast(s)$.\n",
    "    - All optimal policies achieve the optimal action-value function $q_{\\pi_\\ast}(s,a) = q_\\ast(s,a)$.\n",
    "- The theorem says that $$\\pi_\\ast = \\arg\\max_\\pi v_\\pi(s), \\quad \\forall s \\in \\mathcal{S}.$$\n",
    "\n",
    "#### Optimal Policy\n",
    "- An optimal policy can be found by maximizing over $q_\\ast(s,a)$.\n",
    "$$ \\pi_\\ast(a|s) = \\begin{cases} 1 & \\text{if} \\;\\; a = \\arg\\max_a q_\\ast(s,a) \\\\ 0 & \\text{otherwise} \\end{cases} \\quad \\text{or} \\quad \\pi_\\ast(s) = \\arg\\max_a q_\\ast(s,a)$$\n",
    "- $v_\\ast(s) = \\max_a q_\\ast(s,a)$\n",
    "- $v_\\ast(s)$ can be obtained directly from $q_\\ast(s,a)$\n",
    "- $q_\\ast(s,a)$ cannot be obtained directly from $v_\\ast(s)$.\n",
    "\n",
    "### Bellman Optimality Equation \n",
    "\n",
    "\\begin{align*}\n",
    "v_\\ast(s) &= \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma v_\\ast(s')] \\\\\n",
    "q_\\ast(s,a) &= \\sum_{s',r} p(s',r | s,a)[r+\\gamma \\max_{a'}q_\\ast(s',a')] \n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46ed9cd5",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "We want to find an `optimal value function` using iterative algorithm.\n",
    "- Bellman optimal equation\n",
    "    $$V^\\ast(s) = \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V^\\ast(s')]$$\n",
    "\n",
    "`Update Equation` : $V_{k+1}(s) \\leftarrow \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V_k(s')]$\n",
    "\n",
    "1. Initialize $V_0(s) = 0$ for all states $s$.\n",
    "2. Update $V_{k+1}(s)$ iteratively from all $V_k(s')$ (full backup) until convergence.\n",
    "    - synchronous backups: compute $V_{k+1}(s)$ for all $s$ and update simultaneously\n",
    "    - asynchronous backups: compute $V_{k+1}(s)$ for one $s$ and update it immediately\n",
    "3. Compute the optimal policy $\\pi_\\ast$\n",
    "$$ \\pi_\\ast(s) = \\arg\\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V^\\ast(s')]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dadba7b",
   "metadata": {},
   "source": [
    "### Example 1. Driving Car\n",
    "\n",
    "![image](./driving_car.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e776cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be5f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment \n",
    "# 3 states: cool(0), warm(1), overheated(2) \n",
    "# 2 actions: slow(0), fast(1)\n",
    "# Going faster gets double reward with one exception.\n",
    "\n",
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "P = {}\n",
    "P[0] = np.array([[[1,1], [0,1], [0,0]],\n",
    "                [[0.5,1], [0.5,1], [0,0]],\n",
    "                [[0,0], [0,0], [1,0]]])\n",
    "\n",
    "P[1] = np.array([[[0.5,2], [0.5,2], [0,0]],\n",
    "                [[0,2], [0,2], [1,-10]],\n",
    "                [[0,0], [0,0], [1,0]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8255282b",
   "metadata": {},
   "source": [
    "- `Update Equation` : $V_{k+1}(s) \\leftarrow \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V_k(s')]$\n",
    "- `Policy` : $ \\pi_\\ast(s) = \\arg\\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V^\\ast(s')]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37abc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value_ftn_vi(state, values, gamma):         # states: 0, 1, 2\n",
    "    temp_slow = 0\n",
    "    temp_fast = 0\n",
    "    for next_state in range(3):\n",
    "        temp_slow += P[0][state, next_state][0] * (P[0][state, next_state][1] + gamma * values[next_state])\n",
    "        temp_fast += P[1][state, next_state][0] * (P[1][state, next_state][1] + gamma * values[next_state])\n",
    "    \n",
    "    values = [temp_slow, temp_fast]\n",
    "    return max(values), np.argmax(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c7779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage: 0, V[0]= 0.057, V[1]= 0.734, V[2]=0.000, Policy: (1, 0, 1)\n",
      "stage: 1, V[0]= 2.356, V[1]= 1.356, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 2, V[0]= 3.670, V[1]= 2.670, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 3, V[0]= 4.853, V[1]= 3.853, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 4, V[0]= 5.918, V[1]= 4.918, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 5, V[0]= 6.876, V[1]= 5.876, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 6, V[0]= 7.738, V[1]= 6.738, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 7, V[0]= 8.515, V[1]= 7.515, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 8, V[0]= 9.213, V[1]= 8.213, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 9, V[0]= 9.842, V[1]= 8.842, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 10, V[0]= 10.408, V[1]= 9.408, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 11, V[0]= 10.917, V[1]= 9.917, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 12, V[0]= 11.375, V[1]= 10.375, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 13, V[0]= 11.788, V[1]= 10.788, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 14, V[0]= 12.159, V[1]= 11.159, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 15, V[0]= 12.493, V[1]= 11.493, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 16, V[0]= 12.794, V[1]= 11.794, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 17, V[0]= 13.064, V[1]= 12.064, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 18, V[0]= 13.308, V[1]= 12.308, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 19, V[0]= 13.527, V[1]= 12.527, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 20, V[0]= 13.724, V[1]= 12.724, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 21, V[0]= 13.902, V[1]= 12.902, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 22, V[0]= 14.062, V[1]= 13.062, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 23, V[0]= 14.206, V[1]= 13.206, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 24, V[0]= 14.335, V[1]= 13.335, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 25, V[0]= 14.452, V[1]= 13.452, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 26, V[0]= 14.556, V[1]= 13.556, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 27, V[0]= 14.651, V[1]= 13.651, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 28, V[0]= 14.736, V[1]= 13.736, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 29, V[0]= 14.812, V[1]= 13.812, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 30, V[0]= 14.881, V[1]= 13.881, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 31, V[0]= 14.943, V[1]= 13.943, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 32, V[0]= 14.999, V[1]= 13.999, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 33, V[0]= 15.049, V[1]= 14.049, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 34, V[0]= 15.094, V[1]= 14.094, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 35, V[0]= 15.134, V[1]= 14.134, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 36, V[0]= 15.171, V[1]= 14.171, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 37, V[0]= 15.204, V[1]= 14.204, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 38, V[0]= 15.233, V[1]= 14.233, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 39, V[0]= 15.260, V[1]= 14.260, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 40, V[0]= 15.284, V[1]= 14.284, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 41, V[0]= 15.306, V[1]= 14.306, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 42, V[0]= 15.325, V[1]= 14.325, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 43, V[0]= 15.343, V[1]= 14.343, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 44, V[0]= 15.358, V[1]= 14.358, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 45, V[0]= 15.373, V[1]= 14.373, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 46, V[0]= 15.385, V[1]= 14.385, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 47, V[0]= 15.397, V[1]= 14.397, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 48, V[0]= 15.407, V[1]= 14.407, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 49, V[0]= 15.416, V[1]= 14.416, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 50, V[0]= 15.425, V[1]= 14.425, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 51, V[0]= 15.432, V[1]= 14.432, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 52, V[0]= 15.439, V[1]= 14.439, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 53, V[0]= 15.445, V[1]= 14.445, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 54, V[0]= 15.451, V[1]= 14.451, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 55, V[0]= 15.456, V[1]= 14.456, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 56, V[0]= 15.460, V[1]= 14.460, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 57, V[0]= 15.464, V[1]= 14.464, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 58, V[0]= 15.468, V[1]= 14.468, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 59, V[0]= 15.471, V[1]= 14.471, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 60, V[0]= 15.474, V[1]= 14.474, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 61, V[0]= 15.476, V[1]= 14.476, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 62, V[0]= 15.479, V[1]= 14.479, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 63, V[0]= 15.481, V[1]= 14.481, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 64, V[0]= 15.483, V[1]= 14.483, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 65, V[0]= 15.485, V[1]= 14.485, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 66, V[0]= 15.486, V[1]= 14.486, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 67, V[0]= 15.487, V[1]= 14.487, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 68, V[0]= 15.489, V[1]= 14.489, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 69, V[0]= 15.490, V[1]= 14.490, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 70, V[0]= 15.491, V[1]= 14.491, V[2]=0.000, Policy: (1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state (except terminal state) randomly.\n",
    "for i in range(2):\n",
    "    V[i] = np.random.random()\n",
    "V[2] = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.001\n",
    "gamma = 0.9\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(3):\n",
    "    Pi[i] = int(np.random.randint(low=0, high=2, size=1))\n",
    "\n",
    "stage = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    print('stage: %d, V[0]= %.3f, V[1]= %.3f, V[2]=%.3f, Policy: (%d, %d, %d)' %(stage, V[0], V[1], V[2], Pi[0], Pi[1], Pi[2]))\n",
    "    temp = []\n",
    "    for i in range(3):\n",
    "        temp.append(V[i])\n",
    "    for j in range(3):\n",
    "        v = V[j]\n",
    "        V[j], Pi[j] = next_value_ftn_vi(j, temp, gamma)\n",
    "        delta = max(delta, abs(v - V[j]))\n",
    "        \n",
    "    if delta < threshold:\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c1c4cd8",
   "metadata": {},
   "source": [
    "### Example 2. Grid World\n",
    "\n",
    "- States (Total 11 states)\n",
    "    - 0 : (1,1)\n",
    "    - 1 : (2,1)\n",
    "    - 2 : (3,1)\n",
    "    - 3 : (4,1)\n",
    "    - 4 : (1,2)\n",
    "    - 5 : (3,2)\n",
    "    - 6 : (4,2) -> **Terminal state**\n",
    "    - 7 : (1,3)\n",
    "    - 8 : (2,3)\n",
    "    - 9 : (3,3) \n",
    "    - 10 : (4,3) -> **Terminal state**\n",
    "- Actions (Total 4 actions)\n",
    "    - 0 : North\n",
    "    - 1 : South\n",
    "    - 2 : East\n",
    "    - 3 : West\n",
    "- Noisy environment, e.g., $P_{(3,1)(3,2)}^{North} = 0.8$, $P_{(3,1)(2,1)}^{North} = 0.1$, $P_{(3,1)(4,1)}^{North} = 0.1$\n",
    "- Reward\n",
    "    - When an agent reaches the state (4,2), then it gains $-1$ reward.\n",
    "    - When an agent reaches the state (4,3), then it gains $+1$ reward.\n",
    "    - An agent gains small negative reward $c$ for each step, except for the case when the agent reaches the terminal state.\n",
    "\n",
    "\n",
    "![image](./grid_world.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb788fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 4\n",
    "c=-0.4\n",
    "\n",
    "Action = {}\n",
    "Action[0] = 'North'\n",
    "Action[1] = 'South'\n",
    "Action[2] = 'East'\n",
    "Action[3] = 'West'\n",
    "\n",
    "P = {}\n",
    "for i in range(action_dim):\n",
    "    P[i] = np.zeros((state_dim,state_dim,2))\n",
    "\n",
    "# North\n",
    "P[0][0,0] = [0.1, c]\n",
    "P[0][0,1] = [0.1, c]\n",
    "P[0][0,4] = [0.8, c]\n",
    "P[0][1,0] = [0.1, c]\n",
    "P[0][1,1] = [0.8, c]\n",
    "P[0][1,2] = [0.1, c]\n",
    "P[0][2,1] = [0.1, c]\n",
    "P[0][2,5] = [0.8, c]\n",
    "P[0][2,3] = [0.1, c]\n",
    "P[0][3,2] = [0.1, c]\n",
    "P[0][3,3] = [0.1, c]\n",
    "P[0][3,6] = [0.8, -1]\n",
    "P[0][4,4] = [0.2, c]\n",
    "P[0][4,7] = [0.8, c]\n",
    "P[0][5,5] = [0.1, c]\n",
    "P[0][5,9] = [0.8, c]\n",
    "P[0][5,6] = [0.1, -1]\n",
    "P[0][6,6] = [1, 0]\n",
    "P[0][7,7] = [0.9, c]\n",
    "P[0][7,8] = [0.1, c]\n",
    "P[0][8,7] = [0.1, c]\n",
    "P[0][8,8] = [0.8, c]\n",
    "P[0][8,9] = [0.1, c]\n",
    "P[0][9,8] = [0.1, c]\n",
    "P[0][9,9] = [0.8, c]\n",
    "P[0][9,10] = [0.1, 1]\n",
    "P[0][10,10] = [1,0]\n",
    "\n",
    "# South\n",
    "P[1][0,0] = [0.9, c]\n",
    "P[1][0,1] = [0.1, c]\n",
    "P[1][1,0] = [0.1, c]\n",
    "P[1][1,1] = [0.8, c]\n",
    "P[1][1,2] = [0.1, c]\n",
    "P[1][2,1] = [0.1, c]\n",
    "P[1][2,2] = [0.8, c]\n",
    "P[1][2,3] = [0.1, c]\n",
    "P[1][3,2] = [0.1, c]\n",
    "P[1][3,3] = [0.9, c]\n",
    "P[1][4,0] = [0.8, c]\n",
    "P[1][4,4] = [0.2, c]\n",
    "P[1][5,2] = [0.8, c]\n",
    "P[1][5,5] = [0.1, c]\n",
    "P[1][5,6] = [0.1, -1]\n",
    "P[1][6,6] = [1, 0]\n",
    "P[1][7,4] = [0.8, c]\n",
    "P[1][7,7] = [0.1, c]\n",
    "P[1][7,8] = [0.1, c]\n",
    "P[1][8,7] = [0.1, c]\n",
    "P[1][8,8] = [0.8, c]\n",
    "P[1][8,9] = [0.1, c]\n",
    "P[1][9,8] = [0.1, c]\n",
    "P[1][9,9] = [0.8, c]\n",
    "P[1][9,10] = [0.1, 1]\n",
    "P[1][10,10] = [1, 0]\n",
    "\n",
    "# East\n",
    "P[2][0,0] = [0.1, c]\n",
    "P[2][0,1] = [0.8, c]\n",
    "P[2][0,4] = [0.1, c]\n",
    "P[2][1,1] = [0.2, c]\n",
    "P[2][1,2] = [0.8, c]\n",
    "P[2][2,2] = [0.1, c]\n",
    "P[2][2,3] = [0.8, c]\n",
    "P[2][2,5] = [0.1, c]\n",
    "P[2][3,3] = [0.9, c]\n",
    "P[2][3,6] = [0.1, -1]\n",
    "P[2][4,0] = [0.1, c]\n",
    "P[2][4,4] = [0.8, c]\n",
    "P[2][4,7] = [0.1, c]\n",
    "P[2][5,2] = [0.1, c]\n",
    "P[2][5,6] = [0.8, -1]\n",
    "P[2][5,9] = [0.1, c]\n",
    "P[2][6,6] = [1, 0]\n",
    "P[2][7,4] = [0.1, c]\n",
    "P[2][7,7] = [0.1, c]\n",
    "P[2][7,8] = [0.8, c]\n",
    "P[2][8,8] = [0.2, c]\n",
    "P[2][8,9] = [0.8, c]\n",
    "P[2][9,5] = [0.1, c]\n",
    "P[2][9,9] = [0.1, c]\n",
    "P[2][9,10] = [0.8, 1]\n",
    "P[2][10,10] = [1, 0]\n",
    "\n",
    "# West\n",
    "P[3][0,0] = [0.9, c]\n",
    "P[3][0,4] = [0.1, c]\n",
    "P[3][1,0] = [0.8, c]\n",
    "P[3][1,1] = [0.2, c]\n",
    "P[3][2,1] = [0.8, c]\n",
    "P[3][2,2] = [0.1, c]\n",
    "P[3][2,5] = [0.1, c]\n",
    "P[3][3,2] = [0.8, c]\n",
    "P[3][3,3] = [0.1, c]\n",
    "P[3][3,6] = [0.1, -1]\n",
    "P[3][4,0] = [0.1, c]\n",
    "P[3][4,4] = [0.8, c]\n",
    "P[3][4,7] = [0.1, c]\n",
    "P[3][5,2] = [0.1, c]\n",
    "P[3][5,5] = [0.8, c]\n",
    "P[3][5,9] = [0.1, c]\n",
    "P[3][6,6] = [1, 0]\n",
    "P[3][7,4] = [0.1, c]\n",
    "P[3][7,7] = [0.9, c]\n",
    "P[3][8,7] = [0.8, c]\n",
    "P[3][8,8] = [0.2, c]\n",
    "P[3][9,5] = [0.1, c]\n",
    "P[3][9,8] = [0.8, c]\n",
    "P[3][9,9] = [0.1, c]\n",
    "P[3][10,10] = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1010f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value_ftn_vi_2(state, values, gamma, state_dim, action_dim):         # states: 0 ~ 10, actions: 0 ~ 3\n",
    "    temp = {}\n",
    "    for a in range(action_dim):\n",
    "        temp[a] = 0\n",
    "        for next_state in range(state_dim):\n",
    "            temp[a] += P[a][state, next_state][0] * (P[a][state, next_state][1] + gamma * values[next_state])\n",
    "            \n",
    "    # return maximum value and the corresponding action\n",
    "    return max(temp.values()), max(temp, key=temp.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b811b80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1\n",
      "Stage 2\n",
      "Stage 3\n",
      "Stage 4\n",
      "Stage 5\n",
      "Stage 6\n",
      "Stage 7\n",
      "Stage 8\n",
      "Stage 9\n",
      "Stage 10\n",
      "Stage 11\n",
      "Stage 12\n",
      "Stage 13\n",
      "Stage 14\n",
      "V[0] = -1.2002, best action: North\n",
      "V[1] = -0.8989, best action: East\n",
      "V[2] = -0.3989, best action: North\n",
      "V[3] = -0.8657, best action: West\n",
      "V[4] = -0.7379, best action: North\n",
      "V[5] = 0.2219, best action: North\n",
      "V[6] = 0.0000, best action: North\n",
      "V[7] = -0.2378, best action: East\n",
      "V[8] = 0.3247, best action: East\n",
      "V[9] = 0.8247, best action: East\n",
      "V[10] = 0.0000, best action: North\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "for i in range(state_dim):\n",
    "    V[i] = np.random.random()\n",
    "V[6] = 0\n",
    "V[10] = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.0001\n",
    "gamma = 1\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(state_dim):\n",
    "    Pi[i] = int(np.random.randint(low=0, high=action_dim, size=1))\n",
    "\n",
    "stage = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    temp = []\n",
    "    for i in range(state_dim):\n",
    "        temp.append(V[i])\n",
    "    for j in range(state_dim):\n",
    "        v = V[j]\n",
    "        V[j], Pi[j] = next_value_ftn_vi_2(j, temp, gamma, state_dim, action_dim)\n",
    "        delta = max(delta, abs(v - V[j]))\n",
    "        \n",
    "    if delta < threshold:\n",
    "        for i in range(state_dim):\n",
    "            print('V[%d] = %.4f, best action: %s' %(i, V[i], Action[Pi[i]]))\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        print('Stage %d'%stage)\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc65a228",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Policy Iteration repeats `policy evaluation` and `policy improvement` until convergence\n",
    "\n",
    "`Policy Evaluation`: computing $V^\\pi$ from the deterministic policy $\\pi$ using **Bellman Expectation Equation**\n",
    "- `Update Equation`: $V_{k+1}(s) \\leftarrow \\sum_{s',r} p(s',r |s,\\pi(s))[r+\\gamma V_k(s')]$\n",
    "1. Initialize $V_0(s)=0$ for all states $s$.\n",
    "2. Update every $V_{k+1}(s)$ from all $V_k(s')$ until convergence to $V^\\pi$.\n",
    "\n",
    "`Policy Improvement`: improving $\\pi$ to $\\pi'$ by greedy policy based on $V^\\pi$.\n",
    "- `Update Equation`: $\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r+\\gamma V^\\pi(s')] = \\arg\\max_a Q^\\pi(s,a)$\n",
    "\n",
    "Notice that $Q^\\pi(s,\\pi'(s)) \\geq V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee99240b",
   "metadata": {},
   "source": [
    "### Policy Improvement Theorem\n",
    "\n",
    "Let $\\pi$ and $\\pi'$ be two policies. If $Q^\\pi(s,\\pi'(s)) \\geq V^\\pi(s)$ for all $s \\in \\mathcal{S}$, then $V^{\\pi'}(s) \\geq V^\\pi(s)$ for all $s \\in \\mathcal{S}$. This implies that $\\pi'$ is a better policy than $\\pi$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbd8a839",
   "metadata": {},
   "source": [
    "### Example 1. Driving Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e1ed34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c417ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment \n",
    "# 3 states: cool(0), warm(1), overheated(2) \n",
    "# 2 actions: slow(0), fast(1)\n",
    "# Going faster gets double reward with one exception.\n",
    "\n",
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "P = {}\n",
    "P[0] = np.array([[[1,1], [0,1], [0,0]],\n",
    "                [[0.5,1], [0.5,1], [0,0]],\n",
    "                [[0,0], [0,0], [1,0]]])\n",
    "\n",
    "P[1] = np.array([[[0.5,2], [0.5,2], [0,0]],\n",
    "                [[0,2], [0,2], [1,-10]],\n",
    "                [[0,0], [0,0], [1,0]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e310a754",
   "metadata": {},
   "source": [
    "- `Update value`: $V_{k+1}(s) \\leftarrow \\sum_{s',r} p(s',r |s,\\pi(s))[r+\\gamma V_k(s')]$\n",
    "- `Update policy`: $\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r+\\gamma V^\\pi(s')] = \\arg\\max_a Q^\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f04f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value_ftn_pi(state, values, action, gamma):         # states: 0, 1, 2   actions: 0, 1\n",
    "    temp = 0\n",
    "    for i in range(3):\n",
    "        temp += P[action][state, i][0] * (P[action][state, i][1] + gamma * values[i])\n",
    "    return temp\n",
    "\n",
    "def find_action(state, values, gamma):         # states: 0, 1, 2\n",
    "    temp_slow = 0\n",
    "    temp_fast = 0\n",
    "    for next_state in range(3):\n",
    "        temp_slow += P[0][state, next_state][0] * (P[0][state, next_state][1] + gamma * values[next_state])\n",
    "        temp_fast += P[1][state, next_state][0] * (P[1][state, next_state][1] + gamma * values[next_state])\n",
    "    \n",
    "    values = [temp_slow, temp_fast]\n",
    "    return np.argmax(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7904989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage: 0, V[0]= 0.893, V[1]= 0.750, V[2]=0.000, Policy: (0, 1, 1)\n",
      "Evaluation Phase\n",
      "counter: 1, V[0]= 1.803, V[1]= -10.000, V[2]=0.000\n",
      "counter: 2, V[0]= 2.623, V[1]= -10.000, V[2]=0.000\n",
      "counter: 3, V[0]= 3.361, V[1]= -10.000, V[2]=0.000\n",
      "counter: 4, V[0]= 4.025, V[1]= -10.000, V[2]=0.000\n",
      "counter: 5, V[0]= 4.622, V[1]= -10.000, V[2]=0.000\n",
      "counter: 6, V[0]= 5.160, V[1]= -10.000, V[2]=0.000\n",
      "counter: 7, V[0]= 5.644, V[1]= -10.000, V[2]=0.000\n",
      "counter: 8, V[0]= 6.080, V[1]= -10.000, V[2]=0.000\n",
      "counter: 9, V[0]= 6.472, V[1]= -10.000, V[2]=0.000\n",
      "counter: 10, V[0]= 6.824, V[1]= -10.000, V[2]=0.000\n",
      "counter: 11, V[0]= 7.142, V[1]= -10.000, V[2]=0.000\n",
      "counter: 12, V[0]= 7.428, V[1]= -10.000, V[2]=0.000\n",
      "counter: 13, V[0]= 7.685, V[1]= -10.000, V[2]=0.000\n",
      "counter: 14, V[0]= 7.917, V[1]= -10.000, V[2]=0.000\n",
      "counter: 15, V[0]= 8.125, V[1]= -10.000, V[2]=0.000\n",
      "counter: 16, V[0]= 8.312, V[1]= -10.000, V[2]=0.000\n",
      "counter: 17, V[0]= 8.481, V[1]= -10.000, V[2]=0.000\n",
      "counter: 18, V[0]= 8.633, V[1]= -10.000, V[2]=0.000\n",
      "counter: 19, V[0]= 8.770, V[1]= -10.000, V[2]=0.000\n",
      "counter: 20, V[0]= 8.893, V[1]= -10.000, V[2]=0.000\n",
      "counter: 21, V[0]= 9.003, V[1]= -10.000, V[2]=0.000\n",
      "counter: 22, V[0]= 9.103, V[1]= -10.000, V[2]=0.000\n",
      "counter: 23, V[0]= 9.193, V[1]= -10.000, V[2]=0.000\n",
      "counter: 24, V[0]= 9.274, V[1]= -10.000, V[2]=0.000\n",
      "counter: 25, V[0]= 9.346, V[1]= -10.000, V[2]=0.000\n",
      "counter: 26, V[0]= 9.412, V[1]= -10.000, V[2]=0.000\n",
      "counter: 27, V[0]= 9.470, V[1]= -10.000, V[2]=0.000\n",
      "counter: 28, V[0]= 9.523, V[1]= -10.000, V[2]=0.000\n",
      "counter: 29, V[0]= 9.571, V[1]= -10.000, V[2]=0.000\n",
      "counter: 30, V[0]= 9.614, V[1]= -10.000, V[2]=0.000\n",
      "counter: 31, V[0]= 9.653, V[1]= -10.000, V[2]=0.000\n",
      "counter: 32, V[0]= 9.687, V[1]= -10.000, V[2]=0.000\n",
      "counter: 33, V[0]= 9.719, V[1]= -10.000, V[2]=0.000\n",
      "counter: 34, V[0]= 9.747, V[1]= -10.000, V[2]=0.000\n",
      "counter: 35, V[0]= 9.772, V[1]= -10.000, V[2]=0.000\n",
      "counter: 36, V[0]= 9.795, V[1]= -10.000, V[2]=0.000\n",
      "counter: 37, V[0]= 9.815, V[1]= -10.000, V[2]=0.000\n",
      "counter: 38, V[0]= 9.834, V[1]= -10.000, V[2]=0.000\n",
      "counter: 39, V[0]= 9.850, V[1]= -10.000, V[2]=0.000\n",
      "counter: 40, V[0]= 9.865, V[1]= -10.000, V[2]=0.000\n",
      "counter: 41, V[0]= 9.879, V[1]= -10.000, V[2]=0.000\n",
      "counter: 42, V[0]= 9.891, V[1]= -10.000, V[2]=0.000\n",
      "counter: 43, V[0]= 9.902, V[1]= -10.000, V[2]=0.000\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "stage: 1, V[0]= 9.912, V[1]= -10.000, V[2]=0.000, Policy: (0, 0, 0)\n",
      "Evaluation Phase\n",
      "counter: 1, V[0]= 9.921, V[1]= 0.960, V[2]=0.000\n",
      "counter: 2, V[0]= 9.928, V[1]= 5.896, V[2]=0.000\n",
      "counter: 3, V[0]= 9.936, V[1]= 8.121, V[2]=0.000\n",
      "counter: 4, V[0]= 9.942, V[1]= 9.126, V[2]=0.000\n",
      "counter: 5, V[0]= 9.948, V[1]= 9.580, V[2]=0.000\n",
      "counter: 6, V[0]= 9.953, V[1]= 9.788, V[2]=0.000\n",
      "counter: 7, V[0]= 9.958, V[1]= 9.883, V[2]=0.000\n",
      "counter: 8, V[0]= 9.962, V[1]= 9.928, V[2]=0.000\n",
      "counter: 9, V[0]= 9.966, V[1]= 9.951, V[2]=0.000\n",
      "counter: 10, V[0]= 9.969, V[1]= 9.962, V[2]=0.000\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "stage: 2, V[0]= 9.972, V[1]= 9.969, V[2]=0.000, Policy: (1, 0, 0)\n",
      "Evaluation Phase\n",
      "counter: 1, V[0]= 10.974, V[1]= 9.974, V[2]=0.000\n",
      "counter: 2, V[0]= 11.426, V[1]= 10.426, V[2]=0.000\n",
      "counter: 3, V[0]= 11.834, V[1]= 10.834, V[2]=0.000\n",
      "counter: 4, V[0]= 12.200, V[1]= 11.200, V[2]=0.000\n",
      "counter: 5, V[0]= 12.530, V[1]= 11.530, V[2]=0.000\n",
      "counter: 6, V[0]= 12.827, V[1]= 11.827, V[2]=0.000\n",
      "counter: 7, V[0]= 13.095, V[1]= 12.095, V[2]=0.000\n",
      "counter: 8, V[0]= 13.335, V[1]= 12.335, V[2]=0.000\n",
      "counter: 9, V[0]= 13.552, V[1]= 12.552, V[2]=0.000\n",
      "counter: 10, V[0]= 13.746, V[1]= 12.746, V[2]=0.000\n",
      "counter: 11, V[0]= 13.922, V[1]= 12.922, V[2]=0.000\n",
      "counter: 12, V[0]= 14.080, V[1]= 13.080, V[2]=0.000\n",
      "counter: 13, V[0]= 14.222, V[1]= 13.222, V[2]=0.000\n",
      "counter: 14, V[0]= 14.349, V[1]= 13.349, V[2]=0.000\n",
      "counter: 15, V[0]= 14.465, V[1]= 13.465, V[2]=0.000\n",
      "counter: 16, V[0]= 14.568, V[1]= 13.568, V[2]=0.000\n",
      "counter: 17, V[0]= 14.661, V[1]= 13.661, V[2]=0.000\n",
      "counter: 18, V[0]= 14.745, V[1]= 13.745, V[2]=0.000\n",
      "counter: 19, V[0]= 14.821, V[1]= 13.821, V[2]=0.000\n",
      "counter: 20, V[0]= 14.889, V[1]= 13.889, V[2]=0.000\n",
      "counter: 21, V[0]= 14.950, V[1]= 13.950, V[2]=0.000\n",
      "counter: 22, V[0]= 15.005, V[1]= 14.005, V[2]=0.000\n",
      "counter: 23, V[0]= 15.054, V[1]= 14.054, V[2]=0.000\n",
      "counter: 24, V[0]= 15.099, V[1]= 14.099, V[2]=0.000\n",
      "counter: 25, V[0]= 15.139, V[1]= 14.139, V[2]=0.000\n",
      "counter: 26, V[0]= 15.175, V[1]= 14.175, V[2]=0.000\n",
      "counter: 27, V[0]= 15.208, V[1]= 14.208, V[2]=0.000\n",
      "counter: 28, V[0]= 15.237, V[1]= 14.237, V[2]=0.000\n",
      "counter: 29, V[0]= 15.263, V[1]= 14.263, V[2]=0.000\n",
      "counter: 30, V[0]= 15.287, V[1]= 14.287, V[2]=0.000\n",
      "counter: 31, V[0]= 15.308, V[1]= 14.308, V[2]=0.000\n",
      "counter: 32, V[0]= 15.327, V[1]= 14.327, V[2]=0.000\n",
      "counter: 33, V[0]= 15.345, V[1]= 14.345, V[2]=0.000\n",
      "counter: 34, V[0]= 15.360, V[1]= 14.360, V[2]=0.000\n",
      "counter: 35, V[0]= 15.374, V[1]= 14.374, V[2]=0.000\n",
      "counter: 36, V[0]= 15.387, V[1]= 14.387, V[2]=0.000\n",
      "counter: 37, V[0]= 15.398, V[1]= 14.398, V[2]=0.000\n",
      "counter: 38, V[0]= 15.408, V[1]= 14.408, V[2]=0.000\n",
      "Improvement Phase\n",
      "Final Values and an optimal policy. V[0]= 15.4174, V[1]=14.4174, V[2]=0.0000, optimal policy: (1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "for i in range(2):\n",
    "    V[i] = np.random.random()\n",
    "V[2] = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.01\n",
    "gamma = 0.9\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(3):\n",
    "    Pi[i] = int(np.random.randint(low=0, high=2, size=1))\n",
    "\n",
    "stage = 0\n",
    "while True:\n",
    "    print('stage: %d, V[0]= %.3f, V[1]= %.3f, V[2]=%.3f, Policy: (%d, %d, %d)' %(stage, V[0], V[1], V[2], Pi[0], Pi[1], Pi[2]))\n",
    "\n",
    "    ########## Policy Evaluation ############\n",
    "    print('Evaluation Phase')\n",
    "    counter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        temp = []\n",
    "        for i in range(3):\n",
    "            temp.append(V[i])\n",
    "        for j in range(3):\n",
    "            v = V[j]\n",
    "            V[j] = next_value_ftn_pi(j, temp, Pi[j], gamma)\n",
    "            delta = max(delta, abs(v - V[j]))\n",
    "            \n",
    "        if delta < threshold:\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            print('counter: %d, V[0]= %.3f, V[1]= %.3f, V[2]=%.3f' %(counter, V[0], V[1], V[2]))\n",
    "            continue\n",
    "    #########################################\n",
    "\n",
    "    ########## Policy Improvement ###########\n",
    "    print('Improvement Phase')\n",
    "    policy_stable = True\n",
    "    for i in range(3):\n",
    "        old_action = Pi[i]\n",
    "        Pi[i] = find_action(i, V, gamma)\n",
    "        if old_action != Pi[i]:\n",
    "            policy_stable = False\n",
    "    \n",
    "    if policy_stable:\n",
    "        print('Final Values and an optimal policy. V[0]= %.4f, V[1]=%.4f, V[2]=%.4f, optimal policy: (%d, %d, %d)' %(V[0], V[1], V[2], Pi[0], Pi[1], Pi[2]))\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        print('Go back to Evaluation Phase.')\n",
    "        print('#'*50)\n",
    "        continue\n",
    "    #######################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adedca07",
   "metadata": {},
   "source": [
    "### Example 2. Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "738b9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 4\n",
    "c=-0.4\n",
    "\n",
    "Action = {}\n",
    "Action[0] = 'North'\n",
    "Action[1] = 'South'\n",
    "Action[2] = 'East'\n",
    "Action[3] = 'West'\n",
    "\n",
    "P = {}\n",
    "for i in range(action_dim):\n",
    "    P[i] = np.zeros((state_dim,state_dim,2))\n",
    "\n",
    "# North\n",
    "P[0][0,0] = [0.1, c]\n",
    "P[0][0,1] = [0.1, c]\n",
    "P[0][0,4] = [0.8, c]\n",
    "P[0][1,0] = [0.1, c]\n",
    "P[0][1,1] = [0.8, c]\n",
    "P[0][1,2] = [0.1, c]\n",
    "P[0][2,1] = [0.1, c]\n",
    "P[0][2,5] = [0.8, c]\n",
    "P[0][2,3] = [0.1, c]\n",
    "P[0][3,2] = [0.1, c]\n",
    "P[0][3,3] = [0.1, c]\n",
    "P[0][3,6] = [0.8, -1]\n",
    "P[0][4,4] = [0.2, c]\n",
    "P[0][4,7] = [0.8, c]\n",
    "P[0][5,5] = [0.1, c]\n",
    "P[0][5,9] = [0.8, c]\n",
    "P[0][5,6] = [0.1, -1]\n",
    "P[0][6,6] = [1, 0]\n",
    "P[0][7,7] = [0.9, c]\n",
    "P[0][7,8] = [0.1, c]\n",
    "P[0][8,7] = [0.1, c]\n",
    "P[0][8,8] = [0.8, c]\n",
    "P[0][8,9] = [0.1, c]\n",
    "P[0][9,8] = [0.1, c]\n",
    "P[0][9,9] = [0.8, c]\n",
    "P[0][9,10] = [0.1, 1]\n",
    "P[0][10,10] = [1,0]\n",
    "\n",
    "# South\n",
    "P[1][0,0] = [0.9, c]\n",
    "P[1][0,1] = [0.1, c]\n",
    "P[1][1,0] = [0.1, c]\n",
    "P[1][1,1] = [0.8, c]\n",
    "P[1][1,2] = [0.1, c]\n",
    "P[1][2,1] = [0.1, c]\n",
    "P[1][2,2] = [0.8, c]\n",
    "P[1][2,3] = [0.1, c]\n",
    "P[1][3,2] = [0.1, c]\n",
    "P[1][3,3] = [0.9, c]\n",
    "P[1][4,0] = [0.8, c]\n",
    "P[1][4,4] = [0.2, c]\n",
    "P[1][5,2] = [0.8, c]\n",
    "P[1][5,5] = [0.1, c]\n",
    "P[1][5,6] = [0.1, -1]\n",
    "P[1][6,6] = [1, 0]\n",
    "P[1][7,4] = [0.8, c]\n",
    "P[1][7,7] = [0.1, c]\n",
    "P[1][7,8] = [0.1, c]\n",
    "P[1][8,7] = [0.1, c]\n",
    "P[1][8,8] = [0.8, c]\n",
    "P[1][8,9] = [0.1, c]\n",
    "P[1][9,8] = [0.1, c]\n",
    "P[1][9,9] = [0.8, c]\n",
    "P[1][9,10] = [0.1, 1]\n",
    "P[1][10,10] = [1, 0]\n",
    "\n",
    "# East\n",
    "P[2][0,0] = [0.1, c]\n",
    "P[2][0,1] = [0.8, c]\n",
    "P[2][0,4] = [0.1, c]\n",
    "P[2][1,1] = [0.2, c]\n",
    "P[2][1,2] = [0.8, c]\n",
    "P[2][2,2] = [0.1, c]\n",
    "P[2][2,3] = [0.8, c]\n",
    "P[2][2,5] = [0.1, c]\n",
    "P[2][3,3] = [0.9, c]\n",
    "P[2][3,6] = [0.1, -1]\n",
    "P[2][4,0] = [0.1, c]\n",
    "P[2][4,4] = [0.8, c]\n",
    "P[2][4,7] = [0.1, c]\n",
    "P[2][5,2] = [0.1, c]\n",
    "P[2][5,6] = [0.8, -1]\n",
    "P[2][5,9] = [0.1, c]\n",
    "P[2][6,6] = [1, 0]\n",
    "P[2][7,4] = [0.1, c]\n",
    "P[2][7,7] = [0.1, c]\n",
    "P[2][7,8] = [0.8, c]\n",
    "P[2][8,8] = [0.2, c]\n",
    "P[2][8,9] = [0.8, c]\n",
    "P[2][9,5] = [0.1, c]\n",
    "P[2][9,9] = [0.1, c]\n",
    "P[2][9,10] = [0.8, 1]\n",
    "P[2][10,10] = [1, 0]\n",
    "\n",
    "# West\n",
    "P[3][0,0] = [0.9, c]\n",
    "P[3][0,4] = [0.1, c]\n",
    "P[3][1,0] = [0.8, c]\n",
    "P[3][1,1] = [0.2, c]\n",
    "P[3][2,1] = [0.8, c]\n",
    "P[3][2,2] = [0.1, c]\n",
    "P[3][2,5] = [0.1, c]\n",
    "P[3][3,2] = [0.8, c]\n",
    "P[3][3,3] = [0.1, c]\n",
    "P[3][3,6] = [0.1, -1]\n",
    "P[3][4,0] = [0.1, c]\n",
    "P[3][4,4] = [0.8, c]\n",
    "P[3][4,7] = [0.1, c]\n",
    "P[3][5,2] = [0.1, c]\n",
    "P[3][5,5] = [0.8, c]\n",
    "P[3][5,9] = [0.1, c]\n",
    "P[3][6,6] = [1, 0]\n",
    "P[3][7,4] = [0.1, c]\n",
    "P[3][7,7] = [0.9, c]\n",
    "P[3][8,7] = [0.8, c]\n",
    "P[3][8,8] = [0.2, c]\n",
    "P[3][9,5] = [0.1, c]\n",
    "P[3][9,8] = [0.8, c]\n",
    "P[3][9,9] = [0.1, c]\n",
    "P[3][10,10] = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae70020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value_ftn_pi_2(state, values, action, gamma, state_dim):         # states: 0~10   actions: 0~3\n",
    "    temp = 0\n",
    "    for next_state in range(state_dim):\n",
    "        temp += P[action][state, next_state][0] * (P[action][state, next_state][1] + gamma * values[next_state])\n",
    "    return temp\n",
    "\n",
    "def find_action_2(state, values, gamma, state_dim, action_dim):         # states: 0~10\n",
    "    temp = {}\n",
    "    for a in range(action_dim):\n",
    "        temp[a] = 0\n",
    "        for next_state in range(state_dim):\n",
    "            temp[a] += P[a][state, next_state][0] * (P[a][state, next_state][1] + gamma * values[next_state])\n",
    "            \n",
    "    # return the action that maximizes the value\n",
    "    return max(temp, key=temp.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "434d97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Phase Stage 1\n",
      "Current policy: (1, 0, 1, 0, 3, 1, 3, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Evaluation counter 5\n",
      "Evaluation counter 6\n",
      "Evaluation counter 7\n",
      "Evaluation counter 8\n",
      "Evaluation counter 9\n",
      "Evaluation counter 10\n",
      "Evaluation counter 11\n",
      "Evaluation counter 12\n",
      "Evaluation counter 13\n",
      "Evaluation counter 14\n",
      "Evaluation counter 15\n",
      "Evaluation counter 16\n",
      "Evaluation counter 17\n",
      "Evaluation counter 18\n",
      "Evaluation counter 19\n",
      "Evaluation counter 20\n",
      "Evaluation counter 21\n",
      "Evaluation counter 22\n",
      "Evaluation counter 23\n",
      "Evaluation counter 24\n",
      "Evaluation counter 25\n",
      "Evaluation counter 26\n",
      "Evaluation counter 27\n",
      "Evaluation counter 28\n",
      "Evaluation counter 29\n",
      "Evaluation counter 30\n",
      "Evaluation counter 31\n",
      "Evaluation counter 32\n",
      "Evaluation counter 33\n",
      "Evaluation counter 34\n",
      "Evaluation counter 35\n",
      "Evaluation counter 36\n",
      "Evaluation counter 37\n",
      "Evaluation counter 38\n",
      "Evaluation counter 39\n",
      "Evaluation counter 40\n",
      "Evaluation counter 41\n",
      "Evaluation counter 42\n",
      "Evaluation counter 43\n",
      "Evaluation counter 44\n",
      "Evaluation counter 45\n",
      "Evaluation counter 46\n",
      "Evaluation counter 47\n",
      "Evaluation counter 48\n",
      "Evaluation counter 49\n",
      "Evaluation counter 50\n",
      "Evaluation counter 51\n",
      "Evaluation counter 52\n",
      "Evaluation counter 53\n",
      "Evaluation counter 54\n",
      "Evaluation counter 55\n",
      "Evaluation counter 56\n",
      "Evaluation counter 57\n",
      "Evaluation counter 58\n",
      "Evaluation counter 59\n",
      "Evaluation counter 60\n",
      "Evaluation counter 61\n",
      "Evaluation counter 62\n",
      "Evaluation counter 63\n",
      "Evaluation counter 64\n",
      "Evaluation counter 65\n",
      "Evaluation counter 66\n",
      "Evaluation counter 67\n",
      "Evaluation counter 68\n",
      "Evaluation counter 69\n",
      "Evaluation counter 70\n",
      "Evaluation counter 71\n",
      "Evaluation counter 72\n",
      "Evaluation counter 73\n",
      "Evaluation counter 74\n",
      "Evaluation counter 75\n",
      "Evaluation counter 76\n",
      "Evaluation counter 77\n",
      "Evaluation counter 78\n",
      "Evaluation counter 79\n",
      "Evaluation counter 80\n",
      "Evaluation counter 81\n",
      "Evaluation counter 82\n",
      "Evaluation counter 83\n",
      "Evaluation counter 84\n",
      "Evaluation counter 85\n",
      "Evaluation counter 86\n",
      "Evaluation counter 87\n",
      "Evaluation counter 88\n",
      "Evaluation counter 89\n",
      "Evaluation counter 90\n",
      "Evaluation counter 91\n",
      "Evaluation counter 92\n",
      "Evaluation counter 93\n",
      "Evaluation counter 94\n",
      "Evaluation counter 95\n",
      "Evaluation counter 96\n",
      "Evaluation counter 97\n",
      "Evaluation counter 98\n",
      "Evaluation counter 99\n",
      "Evaluation counter 100\n",
      "Evaluation counter 101\n",
      "Evaluation counter 102\n",
      "Evaluation counter 103\n",
      "Evaluation counter 104\n",
      "Evaluation counter 105\n",
      "Evaluation counter 106\n",
      "Evaluation counter 107\n",
      "Evaluation counter 108\n",
      "Evaluation counter 109\n",
      "Evaluation counter 110\n",
      "Evaluation counter 111\n",
      "Evaluation counter 112\n",
      "Evaluation counter 113\n",
      "Evaluation counter 114\n",
      "Evaluation counter 115\n",
      "Evaluation counter 116\n",
      "Evaluation counter 117\n",
      "Evaluation counter 118\n",
      "Evaluation counter 119\n",
      "Evaluation counter 120\n",
      "Evaluation counter 121\n",
      "Evaluation counter 122\n",
      "Evaluation counter 123\n",
      "Evaluation counter 124\n",
      "Evaluation counter 125\n",
      "Evaluation counter 126\n",
      "Evaluation counter 127\n",
      "Evaluation counter 128\n",
      "Evaluation counter 129\n",
      "Evaluation counter 130\n",
      "Evaluation counter 131\n",
      "Evaluation counter 132\n",
      "Evaluation counter 133\n",
      "Evaluation counter 134\n",
      "Evaluation counter 135\n",
      "Evaluation counter 136\n",
      "Evaluation counter 137\n",
      "Evaluation counter 138\n",
      "Evaluation counter 139\n",
      "Evaluation counter 140\n",
      "Evaluation counter 141\n",
      "Evaluation counter 142\n",
      "Evaluation counter 143\n",
      "Evaluation counter 144\n",
      "Evaluation counter 145\n",
      "Evaluation counter 146\n",
      "Evaluation counter 147\n",
      "Evaluation counter 148\n",
      "Evaluation counter 149\n",
      "Evaluation counter 150\n",
      "Evaluation counter 151\n",
      "Evaluation counter 152\n",
      "Evaluation counter 153\n",
      "Evaluation counter 154\n",
      "Evaluation counter 155\n",
      "Evaluation counter 156\n",
      "Evaluation counter 157\n",
      "Evaluation counter 158\n",
      "Evaluation counter 159\n",
      "Evaluation counter 160\n",
      "Evaluation counter 161\n",
      "Evaluation counter 162\n",
      "Evaluation counter 163\n",
      "Evaluation counter 164\n",
      "Evaluation counter 165\n",
      "Evaluation counter 166\n",
      "Evaluation counter 167\n",
      "Evaluation counter 168\n",
      "Evaluation counter 169\n",
      "Evaluation counter 170\n",
      "Evaluation counter 171\n",
      "Evaluation counter 172\n",
      "Evaluation counter 173\n",
      "Evaluation counter 174\n",
      "Evaluation counter 175\n",
      "Evaluation counter 176\n",
      "Evaluation counter 177\n",
      "Evaluation counter 178\n",
      "Evaluation counter 179\n",
      "Evaluation counter 180\n",
      "Evaluation counter 181\n",
      "Evaluation counter 182\n",
      "Evaluation counter 183\n",
      "Evaluation counter 184\n",
      "Evaluation counter 185\n",
      "Evaluation counter 186\n",
      "Evaluation counter 187\n",
      "Evaluation counter 188\n",
      "Evaluation counter 189\n",
      "Evaluation counter 190\n",
      "Evaluation counter 191\n",
      "Evaluation counter 192\n",
      "Evaluation counter 193\n",
      "Evaluation counter 194\n",
      "Evaluation counter 195\n",
      "Evaluation counter 196\n",
      "Evaluation counter 197\n",
      "Evaluation counter 198\n",
      "Evaluation counter 199\n",
      "Evaluation counter 200\n",
      "Evaluation counter 201\n",
      "Evaluation counter 202\n",
      "Evaluation counter 203\n",
      "Evaluation counter 204\n",
      "Evaluation counter 205\n",
      "Evaluation counter 206\n",
      "Evaluation counter 207\n",
      "Evaluation counter 208\n",
      "Evaluation counter 209\n",
      "Evaluation counter 210\n",
      "Evaluation counter 211\n",
      "Evaluation counter 212\n",
      "Evaluation counter 213\n",
      "Evaluation counter 214\n",
      "Evaluation counter 215\n",
      "Evaluation counter 216\n",
      "Evaluation counter 217\n",
      "Evaluation counter 218\n",
      "Evaluation counter 219\n",
      "Evaluation counter 220\n",
      "Evaluation counter 221\n",
      "Evaluation counter 222\n",
      "Evaluation counter 223\n",
      "Evaluation counter 224\n",
      "Evaluation counter 225\n",
      "Evaluation counter 226\n",
      "Evaluation counter 227\n",
      "Evaluation counter 228\n",
      "Evaluation counter 229\n",
      "Evaluation counter 230\n",
      "Evaluation counter 231\n",
      "Evaluation counter 232\n",
      "Evaluation counter 233\n",
      "Evaluation counter 234\n",
      "Evaluation counter 235\n",
      "Evaluation counter 236\n",
      "Evaluation counter 237\n",
      "Evaluation counter 238\n",
      "Evaluation counter 239\n",
      "Evaluation counter 240\n",
      "Evaluation counter 241\n",
      "Evaluation counter 242\n",
      "Evaluation counter 243\n",
      "Evaluation counter 244\n",
      "Evaluation counter 245\n",
      "Evaluation counter 246\n",
      "Evaluation counter 247\n",
      "Evaluation counter 248\n",
      "Evaluation counter 249\n",
      "Evaluation counter 250\n",
      "Evaluation counter 251\n",
      "Evaluation counter 252\n",
      "Evaluation counter 253\n",
      "Evaluation counter 254\n",
      "Evaluation counter 255\n",
      "Evaluation counter 256\n",
      "Evaluation counter 257\n",
      "Evaluation counter 258\n",
      "Evaluation counter 259\n",
      "Evaluation counter 260\n",
      "Evaluation counter 261\n",
      "Evaluation counter 262\n",
      "Evaluation counter 263\n",
      "Evaluation counter 264\n",
      "Evaluation counter 265\n",
      "Evaluation counter 266\n",
      "Evaluation counter 267\n",
      "Evaluation counter 268\n",
      "Evaluation counter 269\n",
      "Evaluation counter 270\n",
      "Evaluation counter 271\n",
      "Evaluation counter 272\n",
      "Evaluation counter 273\n",
      "Evaluation counter 274\n",
      "Evaluation counter 275\n",
      "Evaluation counter 276\n",
      "Evaluation counter 277\n",
      "Evaluation counter 278\n",
      "Evaluation counter 279\n",
      "Evaluation counter 280\n",
      "Evaluation counter 281\n",
      "Evaluation counter 282\n",
      "Evaluation counter 283\n",
      "Evaluation counter 284\n",
      "Evaluation counter 285\n",
      "Evaluation counter 286\n",
      "Evaluation counter 287\n",
      "Evaluation counter 288\n",
      "Evaluation counter 289\n",
      "Evaluation counter 290\n",
      "Evaluation counter 291\n",
      "Evaluation counter 292\n",
      "Evaluation counter 293\n",
      "Evaluation counter 294\n",
      "Evaluation counter 295\n",
      "Evaluation counter 296\n",
      "Evaluation counter 297\n",
      "Evaluation counter 298\n",
      "Evaluation counter 299\n",
      "Evaluation counter 300\n",
      "Evaluation counter 301\n",
      "Evaluation counter 302\n",
      "Evaluation counter 303\n",
      "Evaluation counter 304\n",
      "Evaluation counter 305\n",
      "Evaluation counter 306\n",
      "Evaluation counter 307\n",
      "Evaluation counter 308\n",
      "Evaluation counter 309\n",
      "Evaluation counter 310\n",
      "Evaluation counter 311\n",
      "Evaluation counter 312\n",
      "Evaluation counter 313\n",
      "Evaluation counter 314\n",
      "Evaluation counter 315\n",
      "Evaluation counter 316\n",
      "Evaluation counter 317\n",
      "Evaluation counter 318\n",
      "Evaluation counter 319\n",
      "Evaluation counter 320\n",
      "Evaluation counter 321\n",
      "Evaluation counter 322\n",
      "Evaluation counter 323\n",
      "Evaluation counter 324\n",
      "Evaluation counter 325\n",
      "Evaluation counter 326\n",
      "Evaluation counter 327\n",
      "Evaluation counter 328\n",
      "Evaluation counter 329\n",
      "Evaluation counter 330\n",
      "Evaluation counter 331\n",
      "Evaluation counter 332\n",
      "Evaluation counter 333\n",
      "Evaluation counter 334\n",
      "Evaluation counter 335\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 2\n",
      "Current policy: (0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Evaluation counter 5\n",
      "Evaluation counter 6\n",
      "Evaluation counter 7\n",
      "Evaluation counter 8\n",
      "Evaluation counter 9\n",
      "Evaluation counter 10\n",
      "Evaluation counter 11\n",
      "Evaluation counter 12\n",
      "Evaluation counter 13\n",
      "Evaluation counter 14\n",
      "Evaluation counter 15\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 3\n",
      "Current policy: (0, 3, 0, 0, 0, 0, 0, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Evaluation counter 5\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 4\n",
      "Current policy: (0, 2, 0, 3, 0, 0, 0, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Evaluation counter 5\n",
      "Evaluation counter 6\n",
      "Evaluation counter 7\n",
      "Evaluation counter 8\n",
      "Improvement Phase\n",
      "V[0] = -1.2004, best action: North\n",
      "V[1] = -0.8997, best action: East\n",
      "V[2] = -0.3992, best action: North\n",
      "V[3] = -0.8663, best action: West\n",
      "V[4] = -0.7378, best action: North\n",
      "V[5] = 0.2219, best action: North\n",
      "V[6] = 0.0000, best action: North\n",
      "V[7] = -0.2378, best action: East\n",
      "V[8] = 0.3247, best action: East\n",
      "V[9] = 0.8247, best action: East\n",
      "V[10] = 0.0000, best action: North\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "for i in range(state_dim):\n",
    "    V[i] = np.random.random()\n",
    "V[6] = 0\n",
    "V[10] = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.001\n",
    "gamma = 1\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(state_dim):\n",
    "    Pi[i] = int(np.random.randint(low=0, high=action_dim, size=1))\n",
    "\n",
    "stage = 1\n",
    "while True:\n",
    "    ########## Policy Evaluation ############\n",
    "    print('Evaluation Phase Stage %d' %stage)\n",
    "    print('Current policy:', (Pi[0], Pi[1], Pi[2], Pi[3], Pi[4], Pi[5], Pi[6], Pi[7], Pi[8], Pi[9], Pi[10]))\n",
    "    counter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        temp = []\n",
    "        for i in range(state_dim):\n",
    "            temp.append(V[i])\n",
    "        for j in range(state_dim):\n",
    "            v = V[j]\n",
    "            V[j] = next_value_ftn_pi_2(j, temp, Pi[j], gamma, state_dim)\n",
    "            delta = max(delta, abs(v - V[j]))\n",
    "            \n",
    "        if delta < threshold:\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            print('Evaluation counter %d' %counter)\n",
    "            continue\n",
    "    #########################################\n",
    "\n",
    "    ########## Policy Improvement ###########\n",
    "    print('Improvement Phase')\n",
    "    policy_stable = True\n",
    "    for i in range(state_dim):\n",
    "        old_action = Pi[i]\n",
    "        Pi[i] = find_action_2(i, V, gamma, state_dim, action_dim)\n",
    "        if old_action != Pi[i]:\n",
    "            policy_stable = False\n",
    "    \n",
    "    if policy_stable:\n",
    "        for i in range(state_dim):\n",
    "            print('V[%d] = %.4f, best action: %s' %(i, V[i], Action[Pi[i]]))\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        print('Go back to Evaluation Phase.')\n",
    "        print('#'*50)\n",
    "        continue\n",
    "    #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12e8a974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V[0] = -1.2004, best action: North\n",
      "V[1] = -0.8997, best action: East\n",
      "V[2] = -0.3992, best action: North\n",
      "V[3] = -0.8663, best action: West\n",
      "V[4] = -0.7378, best action: North\n",
      "V[5] = 0.2219, best action: North\n",
      "V[6] = 0.0000, best action: North\n",
      "V[7] = -0.2378, best action: East\n",
      "V[8] = 0.3247, best action: East\n",
      "V[9] = 0.8247, best action: East\n",
      "V[10] = 0.0000, best action: North\n"
     ]
    }
   ],
   "source": [
    "for i in range(state_dim):\n",
    "    print('V[%d] = %.4f, best action: %s' %(i, V[i], Action[Pi[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f04de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MathDLstudy",
   "language": "python",
   "name": "mathdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
