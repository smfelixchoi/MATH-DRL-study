{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c334924d",
   "metadata": {},
   "source": [
    "# From Bellman Equation to Dynamic Programming\n",
    "\n",
    "- 발표자: 석사과정 최선묵"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77a830a2",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "- $\\mathcal{S}$: the state space\n",
    "- $\\mathcal{A}$: the action space\n",
    "- $P$ : the transition probability\n",
    "    - $P_{ss'}^a = P(S_{t+1}=s' | S_t =s, A_t = a)$\n",
    "    - Assume Stationary Markov Process, that is, $P_{ss'}^a$ does not depend on time step $t$.\n",
    "- $R$: the reward function\n",
    "- $\\gamma \\in [0,1]$: discount factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec194fc3",
   "metadata": {},
   "source": [
    "### The GOAL of Reinforcement Learning (Solving MDP)\n",
    "\n",
    "- Given an MDP, we aim to find an optimal `policy` $\\pi_\\ast \\colon \\mathcal{S} \\to \\mathcal{A}$.\n",
    "    - Policy decides which action the agent should choose for each state.\n",
    "    - Policy can be either deterministic or stochastic.\n",
    "        - Deterministic: $\\pi(s) = a \\in \\mathcal{A}$ for each $s \\in \\mathcal{S}$\n",
    "        - Stochastic: $\\pi(a|s)$ a distribution\n",
    "- `Optimal` in what sense?\n",
    "    - An optimal policy maximizes the expected total reward.\n",
    "- **Reward Hypothesis**\n",
    "    - All goals can be described by the maximization of the expected value of the cumulative sum of rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "223d3fc2",
   "metadata": {},
   "source": [
    "- Return $G_t$: the total discounted reward from time step $t$.\n",
    "    - $G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$, $\\gamma \\in [0,1]$\n",
    "- Why discount?\n",
    "    - mathematically convenient (converges if the reward function is bounded)\n",
    "    - uncertainty of the future\n",
    "    - immediate rewards may earn more interset than delayed rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88eef9d8",
   "metadata": {},
   "source": [
    "### Value functions\n",
    "\n",
    "- State-value function $v_\\pi(s)$ for policy $\\pi$\n",
    "$$ v_\\pi(s) = \\mathbb{E}_\\pi [G_t | S_t =s] $$\n",
    "\n",
    "- Action-value function $q_\\pi(s,a)$ for policy $\\pi$\n",
    "$$ q_\\pi(s,a) = \\mathbb{E}_\\pi [G_t | S_t=s, A_t =a]$$\n",
    "\n",
    "- By the definition, $v_\\pi(s) = \\sum_a \\pi(a|s) q_\\pi(s,a)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "553a3031",
   "metadata": {},
   "source": [
    "### Bellman Expectation Equation\n",
    "- a recursive equation decomposing value function into immediate reward and discounted successor value.\n",
    "\\begin{align*}\n",
    "v_\\pi(s) &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\\\\n",
    "         &= \\sum_{a\\in \\mathcal{A}} \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\big[r+\\gamma v_\\pi(s') \\big]\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "q_\\pi(s,a) &= \\mathbb{E}_\\pi [R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t=a)] \\\\\n",
    "           &= \\sum_{s',r} p(s',r|s,a) \\big[ r+ \\gamma \\sum_{a'\\in \\mathcal{A}}\\pi(a'|s')q_\\pi(s',a') \\big]\n",
    "\\end{align*}\n",
    "\n",
    "### Optimal Value Functions and Policy\n",
    "- Optimal state-value function $v_\\ast(s) = \\max_\\pi v_\\pi(s)$\n",
    "- Optimal action-value function $q_\\ast(s,a) = \\max_\\pi q_\\pi(s,a)$\n",
    "- Note that $\\arg\\max_\\pi v_\\pi(s)$ and $\\arg\\max_\\pi v_\\pi(s')$ may not the same.\n",
    "\n",
    "#### Theorem\n",
    "- Define a partial ordering $\\pi' \\geq \\pi$ if $v_\\pi'(s) \\geq v_\\pi(s)$ for all $s$.\n",
    "    - There exists an optimal policy $\\pi_\\ast \\geq \\pi$ for all $\\pi$.\n",
    "    - All optimal policies achieve the optimal state-value function $v_{\\pi_\\ast}(s) = v_\\ast(s)$.\n",
    "    - All optimal policies achieve the optimal action-value function $q_{\\pi_\\ast}(s,a) = q_\\ast(s,a)$.\n",
    "- The theorem says that $$\\pi_\\ast = \\arg\\max_\\pi v_\\pi(s), \\quad \\forall s \\in \\mathcal{S}.$$\n",
    "\n",
    "#### Optimal Policy\n",
    "- An optimal policy can be found by maximizing over $q_\\ast(s,a)$.\n",
    "$$ \\pi_\\ast(a|s) = \\begin{cases} 1 & \\text{if} \\;\\; a = \\arg\\max_a q_\\ast(s,a) \\\\ 0 & \\text{otherwise} \\end{cases} \\quad \\text{or} \\quad \\pi_\\ast(s) = \\arg\\max_a q_\\ast(s,a)$$\n",
    "- $v_\\ast(s) = \\max_a q_\\ast(s,a)$\n",
    "- $v_\\ast(s)$ can be obtained directly from $q_\\ast(s,a)$\n",
    "- $q_\\ast(s,a)$ cannot be obtained directly from $v_\\ast(s)$.\n",
    "\n",
    "### Bellman Optimality Equation \n",
    "\n",
    "\\begin{align*}\n",
    "v_\\ast(s) &= \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma v_\\ast(s')] \\\\\n",
    "q_\\ast(s,a) &= \\sum_{s',r} p(s',r | s,a)[r+\\gamma \\max_{a'}q_\\ast(s',a')] \n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46ed9cd5",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "We want to find an `optimal value function` using iterative algorithm.\n",
    "- Bellman optimal equation\n",
    "    $$V^\\ast(s) = \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V^\\ast(s')]$$\n",
    "\n",
    "`Update Equation` : $V_{k+1}(s) \\leftarrow \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V_k(s')]$\n",
    "\n",
    "1. Initialize $V_0(s) = 0$ for all states $s$.\n",
    "2. Update $V_{k+1}(s)$ iteratively from all $V_k(s')$ (full backup) until convergence ($V_k \\to V^\\ast$ as $k \\to \\infty$).\n",
    "    - synchronous backups: compute $V_{k+1}(s)$ for all $s$ and update simultaneously\n",
    "    - asynchronous backups: compute $V_{k+1}(s)$ for one $s$ and update it immediately\n",
    "3. Compute the optimal policy $\\pi_\\ast$\n",
    "$$ \\pi_\\ast(s) = \\arg\\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V^\\ast(s')]$$\n",
    "\n",
    "The convergence is guaranteed! (using the fact that the operator is a contraction.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dadba7b",
   "metadata": {},
   "source": [
    "### Example 1. Car Driving\n",
    "\n",
    "![image](./car.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e776cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8be5f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment \n",
    "# 3 states: cool(0), warm(1), overheated(2) \n",
    "# 2 actions: slow(0), fast(1)\n",
    "# Going faster gets double reward with one exception.\n",
    "\n",
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state][0] = probability\n",
    "# P[action][state, next_state][1] = reward\n",
    "\n",
    "P = {}\n",
    "P[0] = np.array([[[1,1], [0,1], [0,0]],\n",
    "                [[0.5,1], [0.5,1], [0,0]],\n",
    "                [[0,0], [0,0], [1,0]]])\n",
    "\n",
    "P[1] = np.array([[[0.5,2], [0.5,2], [0,0]],\n",
    "                [[0,2], [0,2], [1,-10]],\n",
    "                [[0,0], [0,0], [1,0]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8255282b",
   "metadata": {},
   "source": [
    "- `Update Equation` : $V_{k+1}(s) \\leftarrow \\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V_k(s')]$\n",
    "- `Policy` : $ \\pi_\\ast(s) = \\arg\\max_a \\sum_{s',r} p(s',r | s,a)[r+\\gamma V^\\ast(s')]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37abc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VI_value_update(state, values, gamma, state_dim, action_dim):         # states: 0, 1, 2\n",
    "    '''\n",
    "    state : 0, 1, ... , state_dim (integer)\n",
    "    values: list of state-values   ex. [V(0), V(1), ... , V(state_dim)]\n",
    "    gamma : discount factor (float between 0 and 1)\n",
    "    state_dim : the number of possible states\n",
    "    action_dim: the number of possible actions\n",
    "    '''\n",
    "    delta = -1e8\n",
    "    for action in range(action_dim):\n",
    "        temp = 0\n",
    "        for next_state in range(state_dim):\n",
    "            temp += P[action][state, next_state][0] * (P[action][state, next_state][1] + gamma * values[next_state])\n",
    "        delta = max(delta, temp)\n",
    "    \n",
    "    return delta\n",
    "\n",
    "def find_policy(state, value_dict, gamma, state_dim, action_dim):\n",
    "    '''\n",
    "    state : 0, 1, ... , state_dim (integer)\n",
    "    value_dict : state-value function (dictionary type): `value_dict[state] = state_value`\n",
    "    gamma : discount factor (float between 0 and 1)\n",
    "    state_dim : the number of possible states\n",
    "    action_dim: the number of possible actions\n",
    "    '''\n",
    "    max = -1e8\n",
    "    index = None\n",
    "\n",
    "    for action in range(action_dim):\n",
    "        temp_value = 0\n",
    "        for next_state in range(state_dim):\n",
    "            temp_value += P[action][state, next_state][0] * (P[action][state, next_state][1] + gamma * value_dict[next_state])\n",
    "        if temp_value > max:\n",
    "            max = temp_value\n",
    "            index = action\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80c7779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage: 0, V[0]= 0.837, V[1]= 0.869, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 1, V[0]= 2.427, V[1]= 1.427, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 2, V[0]= 2.963, V[1]= 1.963, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 3, V[0]= 3.232, V[1]= 2.232, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 4, V[0]= 3.366, V[1]= 2.366, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 5, V[0]= 3.433, V[1]= 2.433, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 6, V[0]= 3.466, V[1]= 2.466, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 7, V[0]= 3.483, V[1]= 2.483, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 8, V[0]= 3.492, V[1]= 2.492, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 9, V[0]= 3.496, V[1]= 2.496, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 10, V[0]= 3.498, V[1]= 2.498, V[2]=0.000, Policy: (1, 0, 0)\n",
      "stage: 11, V[0]= 3.499, V[1]= 2.499, V[2]=0.000, Policy: (1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state (except terminal state) randomly.\n",
    "state_dim = 3\n",
    "for i in range(state_dim):\n",
    "    V[i] = np.random.random()\n",
    "V[state_dim-1] = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.001   # convergence tolerance.\n",
    "gamma = 0.5\n",
    "\n",
    "# Policy\n",
    "action_dim = 2\n",
    "Pi = {}\n",
    "for i in range(state_dim):\n",
    "    Pi[i] = find_policy(i, V, gamma, state_dim, action_dim)\n",
    "\n",
    "stage = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    print('stage: %d, V[0]= %.3f, V[1]= %.3f, V[2]=%.3f, Policy: (%d, %d, %d)' \n",
    "          %(stage, V[0], V[1], V[2], Pi[0], Pi[1], Pi[2]))\n",
    "    \n",
    "    values = [V[0], V[1], V[2]]\n",
    "    \n",
    "    ## Synchronous Value Update\n",
    "    for j in range(state_dim):\n",
    "        v = V[j]\n",
    "        V[j] = VI_value_update(j, values, gamma, state_dim, action_dim)\n",
    "        delta = max(delta, abs(v - V[j]))\n",
    "    \n",
    "    ## Compute Policy based on the current state-values.\n",
    "    for k in range(state_dim):\n",
    "        Pi[k] = find_policy(k, V, gamma, state_dim, action_dim)\n",
    "        \n",
    "    if delta < threshold:\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c1c4cd8",
   "metadata": {},
   "source": [
    "### Example 2. Grid World\n",
    "\n",
    "- States (Total 11 states)\n",
    "    - 0 : (1,1)\n",
    "    - 1 : (2,1)\n",
    "    - 2 : (3,1)\n",
    "    - 3 : (4,1)\n",
    "    - 4 : (1,2)\n",
    "    - 5 : (3,2)\n",
    "    - 6 : (4,2) -> **Terminal state**\n",
    "    - 7 : (1,3)\n",
    "    - 8 : (2,3)\n",
    "    - 9 : (3,3) \n",
    "    - 10 : (4,3) -> **Terminal state**\n",
    "- Actions (Total 4 actions)\n",
    "    - 0 : North\n",
    "    - 1 : South\n",
    "    - 2 : East\n",
    "    - 3 : West\n",
    "- Noisy environment, e.g., $P_{(3,1)(3,2)}^{North} = 0.8$, $P_{(3,1)(2,1)}^{North} = 0.1$, $P_{(3,1)(4,1)}^{North} = 0.1$\n",
    "- Reward\n",
    "    - When an agent reaches the state (4,2), then it gains $-1$ reward.\n",
    "    - When an agent reaches the state (4,3), then it gains $+1$ reward.\n",
    "    - An agent gains small negative reward $c$ for each step, except for the case when the agent reaches the terminal state.\n",
    "\n",
    "\n",
    "![image](./grid_world.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb788fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state][0] = probability\n",
    "# P[action][state, next_state][1] = reward\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 4\n",
    "c=-0.4\n",
    "\n",
    "Action = {}\n",
    "Action[0] = 'North'\n",
    "Action[1] = 'South'\n",
    "Action[2] = 'East'\n",
    "Action[3] = 'West'\n",
    "\n",
    "P = {}\n",
    "for i in range(action_dim):\n",
    "    P[i] = np.zeros((state_dim,state_dim,2))\n",
    "\n",
    "# North\n",
    "P[0][0,0] = [0.1, c] # tries to go west with 10% probability, but stays at (1,1) due to the wall.\n",
    "P[0][0,1] = [0.1, c]\n",
    "P[0][0,4] = [0.8, c]\n",
    "P[0][1,0] = [0.1, c]\n",
    "P[0][1,1] = [0.8, c] # tries to go north with 80% probability, but stays at (2,1) due to the wall.\n",
    "P[0][1,2] = [0.1, c]\n",
    "P[0][2,1] = [0.1, c]\n",
    "P[0][2,5] = [0.8, c]\n",
    "P[0][2,3] = [0.1, c]\n",
    "P[0][3,2] = [0.1, c]\n",
    "P[0][3,3] = [0.1, c] # tries to go east with 10% probability, but stays at (4,1) due to the wall.\n",
    "P[0][3,6] = [0.8, -1]\n",
    "P[0][4,4] = [0.2, c] # tries to go west or east with 20% probability, but stays at (1,2) due to the wall.\n",
    "P[0][4,7] = [0.8, c]\n",
    "P[0][5,5] = [0.1, c] # tries to go west with 10% probability, but stays at (3,2) due to the wall.\n",
    "P[0][5,9] = [0.8, c]\n",
    "P[0][5,6] = [0.1, -1]\n",
    "P[0][6,6] = [1, 0]\n",
    "P[0][7,7] = [0.9, c] # tries to go west and north with 90% probability, but stays at (1,3) due to the wall.\n",
    "P[0][7,8] = [0.1, c]\n",
    "P[0][8,7] = [0.1, c]\n",
    "P[0][8,8] = [0.8, c] # tries to go north with 80% probability, but stays at (2,3) due to the wall.\n",
    "P[0][8,9] = [0.1, c]\n",
    "P[0][9,8] = [0.1, c]\n",
    "P[0][9,9] = [0.8, c] # tries to go north with 80% probability, but stays at (3,3) due to the wall.\n",
    "P[0][9,10] = [0.1, 1]\n",
    "P[0][10,10] = [1,0]\n",
    "\n",
    "# South\n",
    "P[1][0,0] = [0.9, c]\n",
    "P[1][0,1] = [0.1, c]\n",
    "P[1][1,0] = [0.1, c]\n",
    "P[1][1,1] = [0.8, c]\n",
    "P[1][1,2] = [0.1, c]\n",
    "P[1][2,1] = [0.1, c]\n",
    "P[1][2,2] = [0.8, c]\n",
    "P[1][2,3] = [0.1, c]\n",
    "P[1][3,2] = [0.1, c]\n",
    "P[1][3,3] = [0.9, c]\n",
    "P[1][4,0] = [0.8, c]\n",
    "P[1][4,4] = [0.2, c]\n",
    "P[1][5,2] = [0.8, c]\n",
    "P[1][5,5] = [0.1, c]\n",
    "P[1][5,6] = [0.1, -1]\n",
    "P[1][6,6] = [1, 0]\n",
    "P[1][7,4] = [0.8, c]\n",
    "P[1][7,7] = [0.1, c]\n",
    "P[1][7,8] = [0.1, c]\n",
    "P[1][8,7] = [0.1, c]\n",
    "P[1][8,8] = [0.8, c]\n",
    "P[1][8,9] = [0.1, c]\n",
    "P[1][9,8] = [0.1, c]\n",
    "P[1][9,9] = [0.8, c]\n",
    "P[1][9,10] = [0.1, 1]\n",
    "P[1][10,10] = [1, 0]\n",
    "\n",
    "# East\n",
    "P[2][0,0] = [0.1, c]\n",
    "P[2][0,1] = [0.8, c]\n",
    "P[2][0,4] = [0.1, c]\n",
    "P[2][1,1] = [0.2, c]\n",
    "P[2][1,2] = [0.8, c]\n",
    "P[2][2,2] = [0.1, c]\n",
    "P[2][2,3] = [0.8, c]\n",
    "P[2][2,5] = [0.1, c]\n",
    "P[2][3,3] = [0.9, c]\n",
    "P[2][3,6] = [0.1, -1]\n",
    "P[2][4,0] = [0.1, c]\n",
    "P[2][4,4] = [0.8, c]\n",
    "P[2][4,7] = [0.1, c]\n",
    "P[2][5,2] = [0.1, c]\n",
    "P[2][5,6] = [0.8, -1]\n",
    "P[2][5,9] = [0.1, c]\n",
    "P[2][6,6] = [1, 0]\n",
    "P[2][7,4] = [0.1, c]\n",
    "P[2][7,7] = [0.1, c]\n",
    "P[2][7,8] = [0.8, c]\n",
    "P[2][8,8] = [0.2, c]\n",
    "P[2][8,9] = [0.8, c]\n",
    "P[2][9,5] = [0.1, c]\n",
    "P[2][9,9] = [0.1, c]\n",
    "P[2][9,10] = [0.8, 1]\n",
    "P[2][10,10] = [1, 0]\n",
    "\n",
    "# West\n",
    "P[3][0,0] = [0.9, c]\n",
    "P[3][0,4] = [0.1, c]\n",
    "P[3][1,0] = [0.8, c]\n",
    "P[3][1,1] = [0.2, c]\n",
    "P[3][2,1] = [0.8, c]\n",
    "P[3][2,2] = [0.1, c]\n",
    "P[3][2,5] = [0.1, c]\n",
    "P[3][3,2] = [0.8, c]\n",
    "P[3][3,3] = [0.1, c]\n",
    "P[3][3,6] = [0.1, -1]\n",
    "P[3][4,0] = [0.1, c]\n",
    "P[3][4,4] = [0.8, c]\n",
    "P[3][4,7] = [0.1, c]\n",
    "P[3][5,2] = [0.1, c]\n",
    "P[3][5,5] = [0.8, c]\n",
    "P[3][5,9] = [0.1, c]\n",
    "P[3][6,6] = [1, 0]\n",
    "P[3][7,4] = [0.1, c]\n",
    "P[3][7,7] = [0.9, c]\n",
    "P[3][8,7] = [0.8, c]\n",
    "P[3][8,8] = [0.2, c]\n",
    "P[3][9,5] = [0.1, c]\n",
    "P[3][9,8] = [0.8, c]\n",
    "P[3][9,9] = [0.1, c]\n",
    "P[3][10,10] = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b811b80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0\n",
      "Stage 1\n",
      "Stage 2\n",
      "Stage 3\n",
      "Stage 4\n",
      "Stage 5\n",
      "Stage 6\n",
      "Stage 7\n",
      "Stage 8\n",
      "Stage 9\n",
      "Stage 10\n",
      "Stage 11\n",
      "Stage 12\n",
      "Stage 13\n",
      "Stage 14\n",
      "Stage 15\n",
      "V[0] = -1.2001, best action: North\n",
      "V[1] = -0.8989, best action: East\n",
      "V[2] = -0.3989, best action: North\n",
      "V[3] = -0.8657, best action: West\n",
      "V[4] = -0.7378, best action: North\n",
      "V[5] = 0.2219, best action: North\n",
      "V[6] = 0.0000, best action: North\n",
      "V[7] = -0.2378, best action: East\n",
      "V[8] = 0.3247, best action: East\n",
      "V[9] = 0.8247, best action: East\n",
      "V[10] = 0.0000, best action: North\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "for i in range(state_dim):\n",
    "    V[i] = np.random.random()\n",
    "# The state-values at the terminal states are (naturally) defined to be zero.\n",
    "V[6] = 0   \n",
    "V[10] = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.0001   # convergence tolerance.\n",
    "gamma = 1.0\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(state_dim):\n",
    "    Pi[i] = find_policy(i, V, gamma, state_dim, action_dim)\n",
    "\n",
    "stage = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    print('Stage %d'%stage)\n",
    "\n",
    "    values = []\n",
    "    for i in range(state_dim):\n",
    "        values.append(V[i])\n",
    "    \n",
    "    ## Synchronous Value Update\n",
    "    for j in range(state_dim):\n",
    "        v = V[j]\n",
    "        V[j] = VI_value_update(j, values, gamma, state_dim, action_dim)\n",
    "        delta = max(delta, abs(v - V[j]))\n",
    "    \n",
    "    updated_values = []\n",
    "    for i in range(state_dim):\n",
    "        values.append(V[i])\n",
    "\n",
    "    ## Compute Policy based on the current state-values.\n",
    "    for k in range(state_dim):\n",
    "        Pi[k] = find_policy(k, V, gamma, state_dim, action_dim)\n",
    "\n",
    "    if delta < threshold:\n",
    "        for i in range(state_dim):\n",
    "            print('V[%d] = %.4f, best action: %s' %(i, V[i], Action[Pi[i]]))\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc65a228",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Policy Iteration repeats `policy evaluation` and `policy improvement` until convergence\n",
    "\n",
    "`Policy Evaluation`: computing $V^\\pi$ from the deterministic policy $\\pi$ using **Bellman Expectation Equation**\n",
    "- `Update Equation`: $V_{k+1}(s) \\leftarrow \\sum_{s',r} p(s',r |s,\\pi(s))[r+\\gamma V_k(s')]$\n",
    "1. Initialize $V_0(s)=0$ for all states $s$.\n",
    "2. Update every $V_{k+1}(s)$ from all $V_k(s')$ until convergence to $V^\\pi$.\n",
    "\n",
    "`Policy Improvement`: improving $\\pi$ to $\\pi'$ by greedy policy based on $V^\\pi$.\n",
    "- `Update Equation`: $\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r+\\gamma V^\\pi(s')] = \\arg\\max_a Q^\\pi(s,a)$\n",
    "\n",
    "Notice that $Q^\\pi(s,\\pi'(s)) \\geq V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee99240b",
   "metadata": {},
   "source": [
    "### Policy Improvement Theorem\n",
    "\n",
    "Let $\\pi$ and $\\pi'$ be two policies. If $Q^\\pi(s,\\pi'(s)) \\geq V^\\pi(s)$ for all $s \\in \\mathcal{S}$, then $V^{\\pi'}(s) \\geq V^\\pi(s)$ for all $s \\in \\mathcal{S}$. This implies that $\\pi'$ is a better policy than $\\pi$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbd8a839",
   "metadata": {},
   "source": [
    "### Example 1. Driving Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e1ed34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c417ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment \n",
    "# 3 states: cool(0), warm(1), overheated(2) \n",
    "# 2 actions: slow(0), fast(1)\n",
    "# Going faster gets double reward with one exception.\n",
    "\n",
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "P = {}\n",
    "P[0] = np.array([[[1,1], [0,1], [0,0]],\n",
    "                [[0.5,1], [0.5,1], [0,0]],\n",
    "                [[0,0], [0,0], [1,0]]])\n",
    "\n",
    "P[1] = np.array([[[0.5,2], [0.5,2], [0,0]],\n",
    "                [[0,2], [0,2], [1,-10]],\n",
    "                [[0,0], [0,0], [1,0]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e310a754",
   "metadata": {},
   "source": [
    "- `Update value`: $V_{k+1}(s) \\leftarrow \\sum_{s',r} p(s',r |s,\\pi(s))[r+\\gamma V_k(s')]$\n",
    "- `Update policy`: $\\pi'(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r+\\gamma V^\\pi(s')] = \\arg\\max_a Q^\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b6bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_policy(state, value_dict, gamma, state_dim, action_dim):\n",
    "    '''\n",
    "    state : 0, 1, ... , state_dim (integer)\n",
    "    value_dict : state-value function (dictionary type): `value_dict[state] = state_value`\n",
    "    gamma : discount factor (float between 0 and 1)\n",
    "    state_dim : the number of possible states\n",
    "    action_dim: the number of possible actions\n",
    "    '''\n",
    "    max = -1e8\n",
    "    index = None\n",
    "\n",
    "    for action in range(action_dim):\n",
    "        temp_value = 0\n",
    "        for next_state in range(state_dim):\n",
    "            temp_value += P[action][state, next_state][0] * (P[action][state, next_state][1] + gamma * value_dict[next_state])\n",
    "        if temp_value > max:\n",
    "            max = temp_value\n",
    "            index = action\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8f04f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PI_value_update(state, action, values, gamma, state_dim):         # states: 0, 1, 2\n",
    "    '''\n",
    "    state : 0, ... , state_dim (integer)\n",
    "    action: 0, ... , action_dim (integer)\n",
    "    values: list of state-values   ex. [V(0), V(1), ... , V(state_dim)]\n",
    "    gamma : discount factor (float between 0 and 1)\n",
    "    state_dim : the number of possible states\n",
    "    '''\n",
    "    value_temp = 0\n",
    "    for i in range(state_dim):\n",
    "        value_temp += P[action][state, i][0] * (P[action][state, i][1] + gamma * values[i])\n",
    "    return value_temp\n",
    "\n",
    "def find_policy(state, value_dict, gamma, state_dim, action_dim):\n",
    "    '''\n",
    "    state : 0, 1, ... , state_dim (integer)\n",
    "    value_dict : state-value function (dictionary type): `value_dict[state] = state_value`\n",
    "    gamma : discount factor (float between 0 and 1)\n",
    "    state_dim : the number of possible states\n",
    "    action_dim: the number of possible actions\n",
    "    '''\n",
    "    max = -1e8\n",
    "    index = None\n",
    "\n",
    "    for action in range(action_dim):\n",
    "        temp_value = 0\n",
    "        for next_state in range(state_dim):\n",
    "            temp_value += P[action][state, next_state][0] * (P[action][state, next_state][1] + gamma * value_dict[next_state])\n",
    "        if temp_value > max:\n",
    "            max = temp_value\n",
    "            index = action\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7904989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage: 0, V[0]= 0.727, V[1]= 0.178, V[2]=0.000, Policy: (1, 0, 0)\n",
      "Evaluation Phase\n",
      "counter: 1, V[0]= 2.226, V[1]= 1.226, V[2]=0.000\n",
      "counter: 2, V[0]= 2.863, V[1]= 1.863, V[2]=0.000\n",
      "counter: 3, V[0]= 3.182, V[1]= 2.182, V[2]=0.000\n",
      "counter: 4, V[0]= 3.341, V[1]= 2.341, V[2]=0.000\n",
      "counter: 5, V[0]= 3.420, V[1]= 2.420, V[2]=0.000\n",
      "counter: 6, V[0]= 3.460, V[1]= 2.460, V[2]=0.000\n",
      "counter: 7, V[0]= 3.480, V[1]= 2.480, V[2]=0.000\n",
      "Improvement Phase\n",
      "Final Values and an optimal policy. V[0]= 3.4900, V[1]=2.4900, V[2]=0.0000, optimal policy: (1, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "state_dim=3\n",
    "for i in range(state_dim):\n",
    "    V[i] = np.random.random()\n",
    "V[state_dim-1] = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.01\n",
    "gamma = 0.5\n",
    "\n",
    "# Policy\n",
    "action_dim=2\n",
    "Pi = {}\n",
    "for i in range(state_dim):\n",
    "    Pi[i] = find_policy(i, V, gamma, state_dim, action_dim)\n",
    "\n",
    "stage = 0\n",
    "while True:\n",
    "    print('stage: %d, V[0]= %.3f, V[1]= %.3f, V[2]=%.3f, Policy: (%d, %d, %d)' %(stage, V[0], V[1], V[2], Pi[0], Pi[1], Pi[2]))\n",
    "\n",
    "    ########## Policy Evaluation ############\n",
    "    print('Evaluation Phase')\n",
    "    counter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        temp = []\n",
    "        for i in range(3):\n",
    "            temp.append(V[i])\n",
    "        for j in range(3):\n",
    "            v = V[j]\n",
    "            V[j] = PI_value_update(j, Pi[j], temp, gamma, state_dim)\n",
    "            delta = max(delta, abs(v - V[j]))\n",
    "            \n",
    "        if delta < threshold:\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            print('counter: %d, V[0]= %.3f, V[1]= %.3f, V[2]=%.3f' %(counter, V[0], V[1], V[2]))\n",
    "            continue\n",
    "    #########################################\n",
    "\n",
    "    ########## Policy Improvement ###########\n",
    "    print('Improvement Phase')\n",
    "    policy_stable = True\n",
    "    for i in range(state_dim):\n",
    "        old_action = Pi[i]\n",
    "        Pi[i] = find_policy(i, V, gamma, state_dim, action_dim)\n",
    "        if old_action != Pi[i]:\n",
    "            policy_stable = False\n",
    "    \n",
    "    if policy_stable:\n",
    "        print('Final Values and an optimal policy. V[0]= %.4f, V[1]=%.4f, V[2]=%.4f, optimal policy: (%d, %d, %d)' %(V[0], V[1], V[2], Pi[0], Pi[1], Pi[2]))\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        print('Go back to Evaluation Phase.')\n",
    "        print('#'*50)\n",
    "        continue\n",
    "    #######################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adedca07",
   "metadata": {},
   "source": [
    "### Example 2. Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "738b9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Probabilities and Rewards for each action\n",
    "# P[action][state, next_state] = [probability, reward]\n",
    "\n",
    "state_dim = 11\n",
    "action_dim = 4\n",
    "c=-0.4\n",
    "\n",
    "Action = {}\n",
    "Action[0] = 'North'\n",
    "Action[1] = 'South'\n",
    "Action[2] = 'East'\n",
    "Action[3] = 'West'\n",
    "\n",
    "P = {}\n",
    "for i in range(action_dim):\n",
    "    P[i] = np.zeros((state_dim,state_dim,2))\n",
    "\n",
    "# North\n",
    "P[0][0,0] = [0.1, c]\n",
    "P[0][0,1] = [0.1, c]\n",
    "P[0][0,4] = [0.8, c]\n",
    "P[0][1,0] = [0.1, c]\n",
    "P[0][1,1] = [0.8, c]\n",
    "P[0][1,2] = [0.1, c]\n",
    "P[0][2,1] = [0.1, c]\n",
    "P[0][2,5] = [0.8, c]\n",
    "P[0][2,3] = [0.1, c]\n",
    "P[0][3,2] = [0.1, c]\n",
    "P[0][3,3] = [0.1, c]\n",
    "P[0][3,6] = [0.8, -1]\n",
    "P[0][4,4] = [0.2, c]\n",
    "P[0][4,7] = [0.8, c]\n",
    "P[0][5,5] = [0.1, c]\n",
    "P[0][5,9] = [0.8, c]\n",
    "P[0][5,6] = [0.1, -1]\n",
    "P[0][6,6] = [1, 0]\n",
    "P[0][7,7] = [0.9, c]\n",
    "P[0][7,8] = [0.1, c]\n",
    "P[0][8,7] = [0.1, c]\n",
    "P[0][8,8] = [0.8, c]\n",
    "P[0][8,9] = [0.1, c]\n",
    "P[0][9,8] = [0.1, c]\n",
    "P[0][9,9] = [0.8, c]\n",
    "P[0][9,10] = [0.1, 1]\n",
    "P[0][10,10] = [1,0]\n",
    "\n",
    "# South\n",
    "P[1][0,0] = [0.9, c]\n",
    "P[1][0,1] = [0.1, c]\n",
    "P[1][1,0] = [0.1, c]\n",
    "P[1][1,1] = [0.8, c]\n",
    "P[1][1,2] = [0.1, c]\n",
    "P[1][2,1] = [0.1, c]\n",
    "P[1][2,2] = [0.8, c]\n",
    "P[1][2,3] = [0.1, c]\n",
    "P[1][3,2] = [0.1, c]\n",
    "P[1][3,3] = [0.9, c]\n",
    "P[1][4,0] = [0.8, c]\n",
    "P[1][4,4] = [0.2, c]\n",
    "P[1][5,2] = [0.8, c]\n",
    "P[1][5,5] = [0.1, c]\n",
    "P[1][5,6] = [0.1, -1]\n",
    "P[1][6,6] = [1, 0]\n",
    "P[1][7,4] = [0.8, c]\n",
    "P[1][7,7] = [0.1, c]\n",
    "P[1][7,8] = [0.1, c]\n",
    "P[1][8,7] = [0.1, c]\n",
    "P[1][8,8] = [0.8, c]\n",
    "P[1][8,9] = [0.1, c]\n",
    "P[1][9,8] = [0.1, c]\n",
    "P[1][9,9] = [0.8, c]\n",
    "P[1][9,10] = [0.1, 1]\n",
    "P[1][10,10] = [1, 0]\n",
    "\n",
    "# East\n",
    "P[2][0,0] = [0.1, c]\n",
    "P[2][0,1] = [0.8, c]\n",
    "P[2][0,4] = [0.1, c]\n",
    "P[2][1,1] = [0.2, c]\n",
    "P[2][1,2] = [0.8, c]\n",
    "P[2][2,2] = [0.1, c]\n",
    "P[2][2,3] = [0.8, c]\n",
    "P[2][2,5] = [0.1, c]\n",
    "P[2][3,3] = [0.9, c]\n",
    "P[2][3,6] = [0.1, -1]\n",
    "P[2][4,0] = [0.1, c]\n",
    "P[2][4,4] = [0.8, c]\n",
    "P[2][4,7] = [0.1, c]\n",
    "P[2][5,2] = [0.1, c]\n",
    "P[2][5,6] = [0.8, -1]\n",
    "P[2][5,9] = [0.1, c]\n",
    "P[2][6,6] = [1, 0]\n",
    "P[2][7,4] = [0.1, c]\n",
    "P[2][7,7] = [0.1, c]\n",
    "P[2][7,8] = [0.8, c]\n",
    "P[2][8,8] = [0.2, c]\n",
    "P[2][8,9] = [0.8, c]\n",
    "P[2][9,5] = [0.1, c]\n",
    "P[2][9,9] = [0.1, c]\n",
    "P[2][9,10] = [0.8, 1]\n",
    "P[2][10,10] = [1, 0]\n",
    "\n",
    "# West\n",
    "P[3][0,0] = [0.9, c]\n",
    "P[3][0,4] = [0.1, c]\n",
    "P[3][1,0] = [0.8, c]\n",
    "P[3][1,1] = [0.2, c]\n",
    "P[3][2,1] = [0.8, c]\n",
    "P[3][2,2] = [0.1, c]\n",
    "P[3][2,5] = [0.1, c]\n",
    "P[3][3,2] = [0.8, c]\n",
    "P[3][3,3] = [0.1, c]\n",
    "P[3][3,6] = [0.1, -1]\n",
    "P[3][4,0] = [0.1, c]\n",
    "P[3][4,4] = [0.8, c]\n",
    "P[3][4,7] = [0.1, c]\n",
    "P[3][5,2] = [0.1, c]\n",
    "P[3][5,5] = [0.8, c]\n",
    "P[3][5,9] = [0.1, c]\n",
    "P[3][6,6] = [1, 0]\n",
    "P[3][7,4] = [0.1, c]\n",
    "P[3][7,7] = [0.9, c]\n",
    "P[3][8,7] = [0.8, c]\n",
    "P[3][8,8] = [0.2, c]\n",
    "P[3][9,5] = [0.1, c]\n",
    "P[3][9,8] = [0.8, c]\n",
    "P[3][9,9] = [0.1, c]\n",
    "P[3][10,10] = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "434d97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Phase Stage 1\n",
      "Current policy: (2, 0, 0, 3, 3, 3, 1, 3, 0, 2, 1)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Evaluation counter 5\n",
      "Evaluation counter 6\n",
      "Evaluation counter 7\n",
      "Evaluation counter 8\n",
      "Evaluation counter 9\n",
      "Evaluation counter 10\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 2\n",
      "Current policy: (2, 2, 0, 1, 1, 0, 0, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Evaluation counter 4\n",
      "Evaluation counter 5\n",
      "Evaluation counter 6\n",
      "Evaluation counter 7\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 3\n",
      "Current policy: (2, 2, 0, 3, 0, 0, 0, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Evaluation counter 2\n",
      "Evaluation counter 3\n",
      "Improvement Phase\n",
      "Go back to Evaluation Phase.\n",
      "##################################################\n",
      "Evaluation Phase Stage 4\n",
      "Current policy: (0, 2, 0, 3, 0, 0, 0, 2, 2, 2, 0)\n",
      "Evaluation counter 1\n",
      "Improvement Phase\n",
      "V[0] = -0.7383, best action: North\n",
      "V[1] = -0.6832, best action: East\n",
      "V[2] = -0.5372, best action: North\n",
      "V[3] = -0.7104, best action: West\n",
      "V[4] = -0.6681, best action: North\n",
      "V[5] = -0.1688, best action: North\n",
      "V[6] = 0.0000, best action: North\n",
      "V[7] = -0.5032, best action: East\n",
      "V[8] = -0.1116, best action: East\n",
      "V[9] = 0.7490, best action: East\n",
      "V[10] = 0.0000, best action: North\n"
     ]
    }
   ],
   "source": [
    "# Define Value function with a dictionary. \n",
    "V = {}\n",
    "\n",
    "# Initialize the value function for each state.\n",
    "for i in range(state_dim):\n",
    "    V[i] = np.random.random()\n",
    "V[6] = 0\n",
    "V[10] = 0\n",
    "\n",
    "# Set hyperparameters\n",
    "threshold = 0.001\n",
    "gamma = 0.5\n",
    "\n",
    "# Policy\n",
    "Pi = {}\n",
    "for i in range(state_dim):\n",
    "    Pi[i] = int(np.random.randint(low=0, high=action_dim, size=1))\n",
    "\n",
    "stage = 1\n",
    "while True:\n",
    "    ########## Policy Evaluation ############\n",
    "    print('Evaluation Phase Stage %d' %stage)\n",
    "    print('Current policy:', (Pi[0], Pi[1], Pi[2], Pi[3], Pi[4], Pi[5], Pi[6], Pi[7], Pi[8], Pi[9], Pi[10]))\n",
    "    counter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        temp = []\n",
    "        for i in range(state_dim):\n",
    "            temp.append(V[i])\n",
    "        for j in range(state_dim):\n",
    "            v = V[j]\n",
    "            V[j] = PI_value_update(j, Pi[j], temp, gamma, state_dim)\n",
    "            delta = max(delta, abs(v - V[j]))\n",
    "            \n",
    "        if delta < threshold:\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "            print('Evaluation counter %d' %counter)\n",
    "            continue\n",
    "    #########################################\n",
    "\n",
    "    ########## Policy Improvement ###########\n",
    "    print('Improvement Phase')\n",
    "    policy_stable = True\n",
    "    for i in range(state_dim):\n",
    "        old_action = Pi[i]\n",
    "        Pi[i] = find_policy(i, V, gamma, state_dim, action_dim)\n",
    "        if old_action != Pi[i]:\n",
    "            policy_stable = False\n",
    "    \n",
    "    if policy_stable:\n",
    "        for i in range(state_dim):\n",
    "            print('V[%d] = %.4f, best action: %s' %(i, V[i], Action[Pi[i]]))\n",
    "        break\n",
    "    else:\n",
    "        stage += 1\n",
    "        print('Go back to Evaluation Phase.')\n",
    "        print('#'*50)\n",
    "        continue\n",
    "    #######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f04de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MathDLstudy",
   "language": "python",
   "name": "mathdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
